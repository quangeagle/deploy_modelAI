import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from sklearn.preprocessing import MinMaxScaler
from torch.optim.lr_scheduler import ExponentialLR
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import pickle
import os
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List
import uvicorn
from sklearn.metrics import r2_score
import copy
import matplotlib.pyplot as plt
checkpoint_dir = 'model_checkpoints'
os.makedirs(checkpoint_dir, exist_ok=True)

# üîÑ ƒê·ªçc best R2 tr∆∞·ªõc ƒë√≥ n·∫øu c√≥
r2_path = os.path.join(checkpoint_dir, "best_r2_score.txt")
if os.path.exists(r2_path):
    with open(r2_path, 'r') as f:
        best_r2_score = float(f.read().strip())
    print(f"üìå R¬≤ t·ªët nh·∫•t t·ª´ tr∆∞·ªõc: {best_r2_score:.4f}")
else:
    best_r2_score = float('-inf')
    print("üìå Ch∆∞a c√≥ m√¥ h√¨nh t·ªët nh·∫•t tr∆∞·ªõc ƒë√≥, s·∫Ω b·∫Øt ƒë·∫ßu ghi m·ªõi.")


# ========== 1. Dataset ==========
class WalmartDataset(Dataset):
    def __init__(self, sequences, targets):
        self.sequences = sequences
        self.targets = targets

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx):
        return torch.tensor(self.sequences[idx], dtype=torch.float32), torch.tensor(self.targets[idx], dtype=torch.float32)

# ========== 2. Model ==========
class GRUModel(nn.Module):
    def __init__(self, input_size, hidden_size=64, num_layers=1):
        super(GRUModel, self).__init__()
        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)
        self.linear = nn.Linear(hidden_size, 1)

    def forward(self, x):
        out, _ = self.gru(x)
        return self.linear(out[:, -1, :])

# ========== 3. Prepare data ==========
def create_sequences(data, lookback):
    sequences, targets = [], []
    for i in range(len(data) - lookback):
        seq = data[i:i+lookback]
        target = data[i+lookback, 0]  # Weekly_Sales
        sequences.append(seq)
        targets.append(target)
    return np.array(sequences), np.array(targets)

# ========== 4. Train function with rolling + freeze + decay ==========
def train_model_continual_gru(df, lookback=12):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    feature_cols = ['Weekly_Sales', 'Holiday_Flag', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'WeekOfYear', 'Month']
    
    input_scaler = MinMaxScaler()
    target_scaler = MinMaxScaler()
    df[['Weekly_Sales']] = target_scaler.fit_transform(df[['Weekly_Sales']])

    # Scale 7 features c√≤n l·∫°i
    input_features = ['Holiday_Flag', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'WeekOfYear', 'Month']
    input_scaler = MinMaxScaler()
    df[input_features] = input_scaler.fit_transform(df[input_features])

    # ‚úÖ TH√äM TRAIN/TEST SPLIT
    from sklearn.model_selection import train_test_split
    
    # Chu·∫©n b·ªã t·∫•t c·∫£ sequences
    all_sequences = []
    all_targets = []
    
    for store_id in df['Store'].unique():
        store_df = df[df['Store'] == store_id].sort_values('Week_Index')
        data = store_df[feature_cols].values.astype(np.float32)
        X, y = create_sequences(data, lookback)
        all_sequences.extend(X)
        all_targets.extend(y)
    
    # Chia train/test v·ªõi 80/20
    X_train, X_test, y_train, y_test = train_test_split(
        np.array(all_sequences), np.array(all_targets), 
        test_size=0.2, random_state=42
    )
    
    print(f"‚úÖ Train/Test split: {len(X_train)} train, {len(X_test)} test samples")

    store_ids = df['Store'].unique()
    model = GRUModel(input_size=len(feature_cols)).to(device)

    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    scheduler = ExponentialLR(optimizer, gamma=0.95)

    store_loss_log = {}

    # ‚úÖ CH·ªà TRAIN TR√äN TRAIN SET
    for i, store_id in enumerate(store_ids):
        print(f"\n===== Training on Store {store_id} =====")
        store_df = df[df['Store'] == store_id].sort_values('Week_Index')
        data = store_df[feature_cols].values.astype(np.float32)

        X, y = create_sequences(data, lookback)
        dataset = WalmartDataset(X, y)
        loader = DataLoader(dataset, batch_size=16, shuffle=False)

        # üîí Giai ƒëo·∫°n ƒë·∫ßu: ch·ªâ train Linear, gi·ªØ nguy√™n GRU
        if i <= 1:
            for name, param in model.named_parameters():
                if 'gru' in name:
                    param.requires_grad = False
                else:
                    param.requires_grad = True
        # üîì Sau v√†i store: cho ph√©p fine-tune GRU nh·∫π nh√†ng
        else:
            for name, param in model.named_parameters():
                param.requires_grad = True  # fine-tune to√†n b·ªô

        for epoch in range(10):
            model.train()
            losses = []
            for X_batch, y_batch in loader:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                optimizer.zero_grad()
                y_pred = model(X_batch).squeeze()
                loss = criterion(y_pred, y_batch)
                loss.backward()
                optimizer.step()
                losses.append(loss.item())
            mean_loss = np.mean(losses)
            print(f"[Epoch {epoch+1}] Loss: {mean_loss:.4f}")
        
        scheduler.step()
        store_loss_log[store_id] = mean_loss

    print("\n===== Final Store Losses =====")
    for store_id, loss in store_loss_log.items():
        print(f"Store {store_id}: Loss = {loss:.4f}")

    return model, input_scaler, target_scaler, X_test, y_test  # ‚úÖ TR·∫¢ V·ªÄ TEST SET


# ========== 5. Run ==========
df = pd.read_csv("walmart_processed_by_week.csv")
all_preds = []
all_targets = []
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
trained_model, input_scaler, target_scaler, X_test, y_test = train_model_continual_gru(df)

# ‚úÖ EVALUATE CH·ªà TR√äN TEST SET
print("\n===== Evaluating on Test Set =====")
with torch.no_grad():
    # T·∫°o test dataset
    test_dataset = WalmartDataset(X_test, y_test)
    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)
    
    for inputs, targets in test_dataloader:
        inputs = inputs.to(device)
        targets = targets.to(device)

        outputs = trained_model(inputs)
        all_preds.append(outputs.cpu().item())
        all_targets.append(targets.cpu().item())

# Convert to NumPy arrays
preds = np.array(all_preds)
targets = np.array(all_targets)

# Metrics
rmse = np.sqrt(mean_squared_error(targets, preds))
mae = mean_absolute_error(targets, preds)
r2 = r2_score(targets, preds)
# üîΩ Ki·ªÉm tra n·∫øu t·ªët h∆°n th√¨ l∆∞u l·∫°i
if r2 > best_r2_score:
    best_r2_score = r2

    # ‚úÖ L∆∞u m√¥ h√¨nh
    torch.save(trained_model.state_dict(), f"{checkpoint_dir}/best_model.pth")
    
    # ‚úÖ L∆∞u scaler
    with open(f"{checkpoint_dir}/input_scaler.pkl", 'wb') as f:
        pickle.dump(input_scaler, f)
    with open(f"{checkpoint_dir}/target_scaler.pkl", 'wb') as f:
        pickle.dump(target_scaler, f)

    # ‚úÖ Ghi l·∫°i best R¬≤ v√†o file
    with open(r2_path, 'w') as f:
        f.write(str(best_r2_score))

    print(f"‚úÖ L∆∞u m√¥ h√¨nh m·ªõi v·ªõi R¬≤ = {r2:.4f} (t·ªët h∆°n tr∆∞·ªõc ƒë√≥)!")
else:
    print(f"‚ùå R¬≤ = {r2:.4f} kh√¥ng t·ªët h∆°n R¬≤ t·ªët nh·∫•t = {best_r2_score:.4f}, kh√¥ng l∆∞u m√¥ h√¨nh.")

print(f"\nüìä Evaluation on all stores:")
print(f"RMSE: {rmse:.4f}")
print(f"MAE: {mae:.4f}")
print(f"R¬≤: {r2:.4f}")

# ==== 6. ƒê·ªçc d·ªØ li·ªáu 10 tu·∫ßn g·∫ßn nh·∫•t t·ª´ file CSV ====
print("\nüìÇ ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´ file 'input_last_10_weeks.csv'...")

# ƒê·ªçc file
input_df = pd.read_excel("test.xlsx")

# Ki·ªÉm tra ƒë√∫ng s·ªë l∆∞·ª£ng tu·∫ßn v√† c·ªôt
if input_df.shape[0] != 10:
    raise ValueError("‚ùå File ph·∫£i ch·ª©a ƒë√∫ng 10 tu·∫ßn g·∫ßn nh·∫•t!")
if input_df.shape[1] != 8:
    raise ValueError("‚ùå File ph·∫£i c√≥ ƒë√∫ng 8 c·ªôt: Weekly_Sales, Holiday_Flag, Temperature, Fuel_Price, CPI, Unemployment, WeekOfYear, Month")

# Chuy·ªÉn v·ªÅ numpy array
input_weeks = input_df.to_numpy()

print("‚úÖ ƒê√£ load d·ªØ li·ªáu th√†nh c√¥ng:\n", input_df)


# ======= X·ª¨ L√ù INPUT TU·∫¶N M·ªöI =======

feature_cols = ['Weekly_Sales', 'Holiday_Flag', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'WeekOfYear', 'Month']
input_features = ['Holiday_Flag', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'WeekOfYear', 'Month']
weekly_sales_col = ['Weekly_Sales']

input_weeks_df = pd.DataFrame(input_weeks, columns=feature_cols)

# Scale t·ª´ng ph·∫ßn
scaled_inputs_only = input_scaler.transform(input_weeks_df[input_features])
scaled_weekly_sales = target_scaler.transform(input_weeks_df[weekly_sales_col])

# Gh√©p l·∫°i th√†nh ƒë√∫ng input cho model: Weekly_Sales tr∆∞·ªõc + c√°c feature sau
scaled_input = np.concatenate([scaled_weekly_sales, scaled_inputs_only], axis=1)

# D·ª± ƒëo√°n
input_tensor = torch.tensor(scaled_input, dtype=torch.float32).unsqueeze(0).to(device)
trained_model.eval()
with torch.no_grad():
    y_pred_scaled = trained_model(input_tensor).detach().cpu().numpy()

# Inverse transform ƒë·ªÉ l·∫•y gi√° tr·ªã th·∫≠t s·ª±
y_pred_real = target_scaler.inverse_transform(y_pred_scaled)[0, 0]

print("üìâ y_pred (scaled):", y_pred_scaled)
print("‚úÖ y_pred (real):", y_pred_real)
print(f"\nüß† D·ª± ƒëo√°n Weekly_Sales tu·∫ßn ti·∫øp theo: {y_pred_real:.2f}")

print("K·∫øt qu·∫£ ƒë·∫ßu ra sau d·ª± ƒëo√°n (scaled):", y_pred_scaled)
print("Sau khi inverse_transform:", y_pred_real)
# ========== FASTAPI ==========

app = FastAPI()

# ƒê·ªãnh nghƒ©a input schema
class WeeklyInput(BaseModel):
    data: List[List[float]]  # 10 h√†ng x 8 c·ªôt

@app.post("/predict")
def predict_sales(input_data: WeeklyInput):
    try:
        if len(input_data.data) != 10:
            raise HTTPException(status_code=400, detail="‚ùå C·∫ßn ƒë√∫ng 10 tu·∫ßn d·ªØ li·ªáu (10 d√≤ng).")
        
        feature_cols = ['Weekly_Sales', 'Holiday_Flag', 'Temperature', 'Fuel_Price',
                        'CPI', 'Unemployment', 'WeekOfYear', 'Month']
        input_df = pd.DataFrame(input_data.data, columns=feature_cols)
        input_features = feature_cols[1:]
        weekly_sales_col = ['Weekly_Sales']

        # Scale d·ªØ li·ªáu
        scaled_inputs_only = input_scaler.transform(input_df[input_features])
        scaled_weekly_sales = target_scaler.transform(input_df[weekly_sales_col])
        scaled_input = np.concatenate([scaled_weekly_sales, scaled_inputs_only], axis=1)

        # D·ª± ƒëo√°n
        input_tensor = torch.tensor(scaled_input, dtype=torch.float32).unsqueeze(0).to(device)
        trained_model.eval()
        with torch.no_grad():
            y_pred_scaled = trained_model(input_tensor).detach().cpu().numpy()
        y_pred_real = target_scaler.inverse_transform(y_pred_scaled)[0, 0]

        return {"predicted_weekly_sales": round(float(y_pred_real), 2)}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# Ch·∫°y server FastAPI
if __name__ == "__main__":
    uvicorn.run("deep_walmart:app", host="0.0.0.0", port=8000, reload=True)
# === Gom to√†n b·ªô d·ªØ li·ªáu sau khi d·ª± ƒëo√°n ===
X_all = []
y_all = []

with torch.no_grad():
    for store in df['Store'].unique():
        df_store = pd.DataFrame(df[df['Store'] == store]).sort_values(by='Week_Index')
        data = df_store[feature_cols].values.astype(np.float32)
        X, y = create_sequences(data, lookback=12)

        X_all.append(X)
        y_all.append(y)

X_all = np.concatenate(X_all, axis=0)
y_all = np.concatenate(y_all, axis=0)
y_all = y_all.reshape(-1, 1)  # Cho ch·∫Øc

# === H√†m t√≠nh permutation importance ===
def permutation_importance(model, X, y_true, metric=r2_score, n_repeats=3):
    base_pred = model(torch.tensor(X, dtype=torch.float32).to(device)).detach().cpu().numpy()
    base_score = metric(y_true, base_pred)
    
    feature_importance = np.zeros(X.shape[2])

    for col in range(X.shape[2]):
        scores = []
        for _ in range(n_repeats):
            X_permuted = copy.deepcopy(X)
            np.random.shuffle(X_permuted[:, :, col])
            with torch.no_grad():
                pred = model(torch.tensor(X_permuted, dtype=torch.float32).to(device)).cpu().numpy()
            score = metric(y_true, pred)
            scores.append(base_score - score)
        feature_importance[col] = np.mean(scores)
    
    return feature_importance

# === G·ªçi t√≠nh importance
importances = permutation_importance(trained_model, X_all, y_all)

feature_names = ['Weekly_Sales', 'Holiday_Flag', 'Temperature', 'Fuel_Price',
                 'CPI', 'Unemployment', 'WeekOfYear', 'Month']

# === V·∫Ω bi·ªÉu ƒë·ªì
plt.figure(figsize=(10, 5))
plt.bar(feature_names, importances)
plt.xticks(rotation=45)
plt.title("Feature Importance (GRU via Permutation)")
plt.ylabel("Importance Score (Œî R¬≤)")
plt.grid(True)
plt.tight_layout()
plt.savefig("feature_importance_gru.png")
plt.show()

# === In chi ti·∫øt
for i, imp in enumerate(importances):
    print(f"{feature_names[i]}: {imp:.4f}")
