"""
XGBoost stacking v·ªõi ƒë·∫ßu v√†o d·ª± ƒëo√°n GRU cho Walmart sales.

C√°ch d√πng:
- S·ª≠a 2 bi·∫øn ƒë∆∞·ªùng d·∫´n CSV b√™n d∆∞·ªõi: WALMART_CSV_PATH v√† GRU_PRED_CSV_PATH
- Ch·∫°y: python xgb_with_gru_stacking.py

Y√™u c·∫ßu:
- CSV Walmart ƒë√£ x·ª≠ l√Ω c√≥ c√°c c·ªôt:
  ["Holiday_Flag", "Temperature", "Fuel_Price", "CPI", "Unemployment", "Month", "WeekOfYear", "Year", "Weekly_Sales"]
- CSV d·ª± ƒëo√°n GRU c√≥ c√°c c·ªôt: ["date", "gru_pred"]

K·∫øt qu·∫£ ƒë·∫ßu ra (th∆∞ m·ª•c output c·∫°nh file script):
- feature_importance_gain.csv
- feature_importance_permutation.csv
- shap_summary.png
- shap_force.html
"""

from __future__ import annotations

import math
from pathlib import Path
from typing import Tuple, List

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.inspection import permutation_importance
from xgboost import XGBRegressor

# SHAP l√† t√πy ch·ªçn: n·∫øu kh√¥ng c√†i ƒë·∫∑t, s·∫Ω b·ªè qua ph·∫ßn SHAP v√† c·∫£nh b√°o
try:
    import shap  # type: ignore
    SHAP_AVAILABLE = True
except Exception:
    SHAP_AVAILABLE = False


# ======= S·ª¨A ƒê∆Ø·ªúNG D·∫™N T·∫†I ƒê√ÇY =======
WALMART_CSV_PATH = r"E:\TrainAI\Train\walmart_processed_by_week.csv"
GRU_PRED_CSV_PATH = r"E:\TrainAI\Train\Walmart_new\gru_predictions.csv"  # v√≠ d·ª•: c√≥ c·ªôt [date, gru_pred]
# ======================================


def ensure_datetime(series: pd.Series) -> pd.Series:
    """Chuy·ªÉn c·ªôt ng√†y v·ªÅ datetime, an to√†n v·ªõi nhi·ªÅu ƒë·ªãnh d·∫°ng."""
    try:
        return pd.to_datetime(series)
    except Exception:
        # Th·ª≠ parse v·ªõi dayfirst ƒë·ªÉ an to√†n h∆°n
        return pd.to_datetime(series, dayfirst=True, errors="coerce")


def build_date_from_year_week(df: pd.DataFrame, year_col: str = "Year", week_col: str = "WeekOfYear") -> pd.Series:
    """T·∫°o c·ªôt date (th·ª© Hai c·ªßa tu·∫ßn ISO) t·ª´ c·ªôt nƒÉm v√† tu·∫ßn.
    Ch·∫≠m nh∆∞ng an to√†n cho c·ª° d·ªØ li·ªáu v·ª´a.
    """
    def to_monday(row):
        try:
            return pd.Timestamp.fromisocalendar(int(row[year_col]), int(row[week_col]), 1)
        except Exception:
            return pd.NaT

    return df[[year_col, week_col]].apply(to_monday, axis=1)


def read_and_merge(walmart_csv: str | Path, gru_csv: str | Path) -> pd.DataFrame:
    """ƒê·ªçc hai file CSV v√† merge theo c·ªôt date.
    - Walmart CSV: ch·ª©a ƒë·∫∑c tr∆∞ng v√† Weekly_Sales, c√≥ th·ªÉ c√≥ c·ªôt Date ho·∫∑c (Year, WeekOfYear)
    - GRU CSV: ph·∫£i c√≥ c·ªôt date v√† gru_pred
    """
    walmart_df = pd.read_csv(walmart_csv)

    # N·∫øu file GRU kh√¥ng t·ªìn t·∫°i, t·∫°o fallback d·ª± ƒëo√°n ƒë∆°n gi·∫£n t·ª´ doanh thu tu·∫ßn tr∆∞·ªõc (naive persistence)
    gru_csv_path = Path(gru_csv)
    if not gru_csv_path.exists():
        print("[C·∫¢NH B√ÅO] Kh√¥ng t√¨m th·∫•y file GRU d·ª± ƒëo√°n:", gru_csv_path)
        print("            S·∫Ω t·∫°o t·∫°m 'gru_pred' = trung b√¨nh doanh thu c·ªßa tu·∫ßn tr∆∞·ªõc (naive), b·∫°n n√™n thay b·∫±ng file GRU th·∫≠t.")
        # Chu·∫©n h√≥a 'date' cho walmart_df tr∆∞·ªõc khi t·∫°o gru_df t·∫°m
        if "date" in walmart_df.columns:
            walmart_df["date"] = ensure_datetime(walmart_df["date"])
        elif "Date" in walmart_df.columns:
            walmart_df["date"] = ensure_datetime(walmart_df["Date"])
        elif {"Year", "WeekOfYear"}.issubset(walmart_df.columns):
            walmart_df["date"] = build_date_from_year_week(walmart_df, "Year", "WeekOfYear")
        else:
            raise ValueError("Walmart CSV c·∫ßn c√≥ c·ªôt 'date' ho·∫∑c 'Date' ho·∫∑c c·∫∑p (Year, WeekOfYear).")

        # T·∫°o d·ª± ƒëo√°n m·ª©c aggregate theo tu·∫ßn: trung b√¨nh c√°c store c√πng ng√†y r·ªìi shift 1 tu·∫ßn
        date_avg = walmart_df.groupby("date")["Weekly_Sales"].mean().sort_index()
        gru_series = date_avg.shift(1).bfill()
        gru_df = gru_series.reset_index()
        gru_df.columns = ["date", "gru_pred"]
    else:
        gru_df = pd.read_csv(gru_csv_path)

    # Chu·∫©n h√≥a c·ªôt ng√†y ·ªü walmart_df ‚Üí 'date'
    if "date" in walmart_df.columns:
        walmart_df["date"] = ensure_datetime(walmart_df["date"])
    elif "Date" in walmart_df.columns:
        walmart_df["date"] = ensure_datetime(walmart_df["Date"])
    elif {"Year", "WeekOfYear"}.issubset(walmart_df.columns):
        walmart_df["date"] = build_date_from_year_week(walmart_df, "Year", "WeekOfYear")
    else:
        raise ValueError("Walmart CSV c·∫ßn c√≥ c·ªôt 'date' ho·∫∑c 'Date' ho·∫∑c c·∫∑p (Year, WeekOfYear).")

    # Chu·∫©n h√≥a c·ªôt ng√†y ·ªü gru_df
    if "gru_pred" not in gru_df.columns:
        raise ValueError("GRU CSV c·∫ßn c√≥ c·ªôt 'gru_pred'.")

    if "date" in gru_df.columns:
        gru_df["date"] = ensure_datetime(gru_df["date"])
    else:
        # N·∫øu kh√¥ng c√≥ c·ªôt date, t·ª± √°nh x·∫° d·ª± ƒëo√°n v√†o c√°c ng√†y cu·ªëi c√πng t∆∞∆°ng ·ª©ng
        print("[C·∫¢NH B√ÅO] GRU CSV kh√¥ng c√≥ c·ªôt 'date'. S·∫Ω gh√©p d·ª± ƒëo√°n theo c√°c ng√†y cu·ªëi c√πng trong Walmart CSV.")
        # L·∫•y danh s√°ch ng√†y duy nh·∫•t (chu·ªói th·ªùi gian theo tu·∫ßn), s·∫Øp x·∫øp tƒÉng d·∫ßn
        unique_dates = walmart_df["date"].dropna().drop_duplicates().sort_values().reset_index(drop=True)
        p = len(gru_df)
        if p > len(unique_dates):
            raise ValueError("S·ªë d√≤ng GRU l·ªõn h∆°n s·ªë ng√†y trong Walmart CSV. Kh√¥ng th·ªÉ gh√©p t·ª± ƒë·ªông.")
        mapped_dates = unique_dates.iloc[-p:].reset_index(drop=True)
        tmp = pd.DataFrame({"date": mapped_dates, "gru_pred": gru_df["gru_pred"].values})
        gru_df = tmp

    # Merge theo date (inner ƒë·ªÉ ƒë·∫£m b·∫£o d·ªØ li·ªáu chung)
    merged = pd.merge(walmart_df, gru_df[["date", "gru_pred"]], on="date", how="inner")
    merged = merged.sort_values("date").reset_index(drop=True)

    # Ki·ªÉm tra c·ªôt b·∫Øt bu·ªôc
    required_cols = [
        "Holiday_Flag", "Temperature", "Fuel_Price", "CPI", "Unemployment",
        "Month", "WeekOfYear", "Year", "Weekly_Sales", "gru_pred"
    ]
    missing = [c for c in required_cols if c not in merged.columns]
    if missing:
        raise ValueError(f"Thi·∫øu c·ªôt b·∫Øt bu·ªôc sau khi merge: {missing}")

    # Lo·∫°i b·ªè NA
    merged = merged.dropna(subset=required_cols)
    return merged


def train_test_time_split(df: pd.DataFrame, feature_cols: List[str], target_col: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:
    """T·∫°o t·∫≠p train/test theo th·ªùi gian (80/20), gi·ªØ nguy√™n th·ª© t·ª± (shuffle=False)."""
    num_rows = len(df)
    split_idx = int(num_rows * 0.8)

    X = df[feature_cols]
    y = df[target_col]

    X_train = X.iloc[:split_idx].copy()
    y_train = y.iloc[:split_idx].copy()
    X_test = X.iloc[split_idx:].copy()
    y_test = y.iloc[split_idx:].copy()

    return X_train, X_test, y_train, y_test


def train_xgb(X_train: pd.DataFrame, y_train: pd.Series) -> XGBRegressor:
    """Hu·∫•n luy·ªán XGBRegressor theo tham s·ªë c∆° b·∫£n y√™u c·∫ßu."""
    model = XGBRegressor(
        n_estimators=500,
        learning_rate=0.05,
        max_depth=5,
        subsample=0.8,
        colsample_bytree=0.8,
        objective="reg:squarederror",
        n_jobs=-1,
        random_state=42,
    )
    model.fit(X_train, y_train)
    return model


def evaluate_model(model: XGBRegressor, X_test: pd.DataFrame, y_test: pd.Series) -> Tuple[float, float, float, np.ndarray]:
    """T√≠nh R¬≤, RMSE, MAE v√† tr·∫£ l·∫°i y_pred."""
    y_pred = model.predict(X_test)
    r2 = r2_score(y_test, y_pred)
    rmse = math.sqrt(mean_squared_error(y_test, y_pred))
    mae = mean_absolute_error(y_test, y_pred)
    return r2, rmse, mae, y_pred


def xgb_feature_importance_gain(model: XGBRegressor, feature_names: List[str]) -> pd.Series:
    """L·∫•y gain importance t·ª´ booster v√† map v·ªÅ t√™n c·ªôt th·ª±c."""
    booster = model.get_booster()
    gain_score = booster.get_score(importance_type="gain")
    fmap = {f"f{i}": name for i, name in enumerate(feature_names)}
    gain_series = pd.Series({fmap.get(k, k): v for k, v in gain_score.items()})
    # B·ªï sung c·ªôt kh√¥ng xu·∫•t hi·ªán v·ªÅ 0 ƒë·ªÉ c√≥ ƒë·ªß t·∫•t c·∫£ features
    gain_series = gain_series.reindex(feature_names).fillna(0.0).sort_values(ascending=False)
    return gain_series


def xgb_permutation_importance(model: XGBRegressor, X_test: pd.DataFrame, y_test: pd.Series) -> pd.Series:
    """T√≠nh permutation importance (mean decrease in score)."""
    result = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)
    perm_series = pd.Series(result.importances_mean, index=X_test.columns).sort_values(ascending=False)
    return perm_series


def save_importance_outputs(gain_series: pd.Series, perm_series: pd.Series, out_dir: Path) -> None:
    """L∆∞u CSV v√† bi·ªÉu ƒë·ªì PNG cho importance."""
    out_dir.mkdir(parents=True, exist_ok=True)

    gain_df = gain_series.reset_index()
    gain_df.columns = ["feature", "gain_importance"]
    perm_df = perm_series.reset_index()
    perm_df.columns = ["feature", "permutation_importance"]

    gain_csv = out_dir / "feature_importance_gain.csv"
    perm_csv = out_dir / "feature_importance_permutation.csv"
    gain_df.to_csv(gain_csv, index=False)
    perm_df.to_csv(perm_csv, index=False)

    # V·∫Ω bi·ªÉu ƒë·ªì
    sns.set_theme(style="whitegrid")

    plt.figure(figsize=(8, max(3, len(gain_series) * 0.4)))
    gain_series.sort_values().plot(kind="barh", color="#1f77b4")
    plt.title("XGBoost Feature Importance (Gain)")
    plt.xlabel("Gain")
    plt.tight_layout()
    plt.savefig(out_dir / "xgb_feature_importance_gain.png", dpi=300, bbox_inches="tight")
    plt.close()

    plt.figure(figsize=(8, max(3, len(perm_series) * 0.4)))
    perm_series.sort_values().plot(kind="barh", color="#ff7f0e")
    plt.title("Permutation Importance (XGBoost)")
    plt.xlabel("Mean importance")
    plt.tight_layout()
    plt.savefig(out_dir / "xgb_feature_importance_permutation.png", dpi=300, bbox_inches="tight")
    plt.close()


def shap_outputs(model: XGBRegressor, X_test: pd.DataFrame, out_dir: Path) -> None:
    """T√≠nh SHAP values v√† l∆∞u summary plot (png) + force plot (html).
    S·∫Ω b·ªè qua n·∫øu kh√¥ng c√≥ SHAP.
    """
    if not SHAP_AVAILABLE:
        print("[C·∫¢NH B√ÅO] Th∆∞ vi·ªán shap ch∆∞a c√†i ƒë·∫∑t. B·ªè qua ph·∫ßn SHAP.")
        return

    out_dir.mkdir(parents=True, exist_ok=True)

    try:
        # ∆Øu ti√™n TreeExplainer cho m√¥ h√¨nh c√¢y
        explainer = shap.TreeExplainer(model)
        shap_values = explainer.shap_values(X_test)
        expected_value = explainer.expected_value
    except Exception:
        # Fallback API m·ªõi
        explainer = shap.Explainer(model)
        shap_values_obj = explainer(X_test)
        shap_values = shap_values_obj.values
        expected_value = shap_values_obj.base_values

    # Summary plot
    plt.figure(figsize=(10, 6))
    shap.summary_plot(shap_values, X_test, show=False)
    plt.tight_layout()
    plt.savefig(out_dir / "shap_summary.png", dpi=300, bbox_inches="tight")
    plt.close()

    # Force plot cho 1 quan s√°t ƒë·∫ßu ti√™n c·ªßa test
    try:
        force = shap.force_plot(expected_value, shap_values[0, :], X_test.iloc[0, :], matplotlib=False)
        shap.save_html(str(out_dir / "shap_force.html"), force)
    except Exception:
        # Fallback cho d·∫°ng object c·ªßa Explainer m·ªõi
        try:
            force = shap.force_plot(expected_value, shap_values[0], X_test.iloc[0, :], matplotlib=False)
            shap.save_html(str(out_dir / "shap_force.html"), force)
        except Exception as e:
            print(f"[SHAP] Kh√¥ng th·ªÉ l∆∞u force plot: {e}")


def main():
    # 1) ƒê·ªçc v√† merge d·ªØ li·ªáu
    merged = read_and_merge(WALMART_CSV_PATH, GRU_PRED_CSV_PATH)

    # 2) X√°c ƒë·ªãnh ƒë·∫∑c tr∆∞ng + target
    feature_cols = [
        "Holiday_Flag", "Temperature", "Fuel_Price", "CPI", "Unemployment",
        "Month", "WeekOfYear", "Year", "gru_pred"
    ]
    target_col = "Weekly_Sales"

    # 3) T·∫°o train/test split theo th·ªùi gian
    X_train, X_test, y_train, y_test = train_test_time_split(merged, feature_cols, target_col)

    # 4) Hu·∫•n luy·ªán XGBoost
    model = train_xgb(X_train, y_train)

    # 5) ƒê√°nh gi√°
    r2, rmse, mae, y_pred = evaluate_model(model, X_test, y_test)

    # 6) Importance
    gain_series = xgb_feature_importance_gain(model, feature_cols)
    perm_series = xgb_permutation_importance(model, X_test, y_test)

    # 7) SHAP
    out_dir = Path(__file__).resolve().parent / "output"
    shap_outputs(model, X_test, out_dir)

    # 8) L∆∞u importance outputs
    save_importance_outputs(gain_series, perm_series, out_dir)

    # 9) In k·∫øt qu·∫£
    print("=" * 60)
    print("üèÅ Hi·ªáu nƒÉng m√¥ h√¨nh (Test set)")
    print(f"R¬≤: {r2:.4f} | RMSE: {rmse:,.2f} | MAE: {mae:,.2f}")

    print("\nüî• Top 10 features (Gain):")
    top10 = gain_series.head(10)
    for name, val in top10.items():
        print(f"- {name}: {val:.6f}")

    print("\nüìÅ ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o th∆∞ m·ª•c:", out_dir)
    print("- feature_importance_gain.csv")
    print("- feature_importance_permutation.csv")
    if SHAP_AVAILABLE:
        print("- shap_summary.png")
        print("- shap_force.html")
    else:
        print("(B·ªè qua SHAP do thi·∫øu th∆∞ vi·ªán shap)")


if __name__ == "__main__":
    main()


