# Improved GRU Sales Prediction Model
# Tinh ch·ªânh ƒë·ªÉ d·ª± ƒëo√°n t·ªët h∆°n cho c√°c xu h∆∞·ªõng kh√°c nhau

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# C√†i ƒë·∫∑t device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"üîß S·ª≠ d·ª•ng device: {device}")

# ========== 1. IMPROVED GRU MODEL ==========
class ImprovedGRUSalesPredictor(nn.Module):
    def __init__(self, input_size, hidden_size=256, num_layers=3, dropout=0.3):
        super(ImprovedGRUSalesPredictor, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # GRU layers v·ªõi attention
        self.gru = nn.GRU(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0,
            bidirectional=True  # Bidirectional ƒë·ªÉ capture patterns t·ªët h∆°n
        )
        
        # Attention mechanism
        self.attention = nn.MultiheadAttention(
            embed_dim=hidden_size * 2,  # Bidirectional
            num_heads=8,
            dropout=dropout
        )
        
        # Additional layers (T·ªêI ∆ØU)
        self.fc1 = nn.Linear(hidden_size * 4, hidden_size)  # 4 v√¨ combined = avg_pool + max_pool
        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)
        self.fc3 = nn.Linear(hidden_size // 2, 1)
        
        # Dropout layers
        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(hidden_size * 2)
        
        # Activation functions
        self.relu = nn.ReLU()
        self.tanh = nn.Tanh()
    
    def forward(self, x):
        # x shape: (batch_size, sequence_length, input_size)
        
        # GRU processing
        gru_out, _ = self.gru(x)  # (batch_size, seq_len, hidden_size * 2)
        
        # Apply layer normalization
        gru_out = self.layer_norm(gru_out)
        
        # Self-attention mechanism
        gru_out = gru_out.transpose(0, 1)  # (seq_len, batch_size, hidden_size * 2)
        attn_out, _ = self.attention(gru_out, gru_out, gru_out)
        attn_out = attn_out.transpose(0, 1)  # (batch_size, seq_len, hidden_size * 2)
        
        # Global average pooling + max pooling
        avg_pool = torch.mean(attn_out, dim=1)
        max_pool, _ = torch.max(attn_out, dim=1)
        
        # Combine pooling results
        combined = torch.cat([avg_pool, max_pool], dim=1)
        
        # Fully connected layers
        out = self.fc1(combined)
        out = self.relu(out)
        out = self.dropout(out)
        
        out = self.fc2(out)
        out = self.relu(out)
        out = self.dropout(out)
        
        out = self.fc3(out)
        
        return out

# ========== 2. ENHANCED DATASET CREATION ==========
def create_balanced_sequential_data(df, lookback=10):
    """
    T·∫°o balanced dataset v·ªõi c√°c xu h∆∞·ªõng kh√°c nhau
    """
    print(f"\nüîÑ T·∫°o balanced sequential data v·ªõi lookback={lookback} tu·∫ßn...")
    
    all_sequences = []
    all_targets = []
    trend_labels = []  # ƒê·ªÉ track xu h∆∞·ªõng
    target_dates = []  # Ng√†y t∆∞∆°ng ·ª©ng v·ªõi m·ªói target (NaT cho d·ªØ li·ªáu synthetic)
    target_store_ids = []  # Store t∆∞∆°ng ·ª©ng (None cho synthetic)
    
    # 1. Real data t·ª´ dataset
    for store_id in df['Store'].unique():
        store_df = df[df['Store'] == store_id].sort_values('Date')
        sales_data = store_df['Weekly_Sales'].values.astype(np.float32)
        
        for i in range(len(sales_data) - lookback):
            sequence = sales_data[i:i+lookback]
            target = sales_data[i+lookback]
            
            # Determine trend
            if sequence[-1] > sequence[0]:
                trend = "increasing"
            elif sequence[-1] < sequence[0]:
                trend = "decreasing"
            else:
                trend = "stable"
            
            all_sequences.append(sequence.reshape(-1, 1))
            all_targets.append(target)
            trend_labels.append(trend)
            # L∆∞u ng√†y & store c·ªßa tu·∫ßn target (th·ª±c)
            target_dates.append(store_df['Date'].iloc[i+lookback])
            target_store_ids.append(store_id)
    
    # 2. Synthetic data ƒë·ªÉ balance dataset (GI·∫¢M S·ªê L∆Ø·ª¢NG)
    print("üìä T·∫°o synthetic data ƒë·ªÉ balance dataset...")
    
    # Increasing trends (GI·∫¢M T·ª™ 1000 XU·ªêNG 200)
    for _ in range(200):
        start_value = np.random.uniform(500000, 1500000)
        sequence = []
        for i in range(lookback):
            # T·∫°o growth rate realistic h∆°n
            growth_rate = np.random.uniform(0.01, 0.08)  # Gi·∫£m t·ª´ 2-15% xu·ªëng 1-8%
            if i == 0:
                sequence.append(start_value)
            else:
                sequence.append(sequence[-1] * (1 + growth_rate))
        
        target = sequence[-1] * (1 + np.random.uniform(0.01, 0.08))
        
        all_sequences.append(np.array(sequence).reshape(-1, 1))
        all_targets.append(target)
        trend_labels.append("increasing")
        target_dates.append(pd.NaT)
        target_store_ids.append(None)
    
    # Decreasing trends (GI·∫¢M T·ª™ 1000 XU·ªêNG 200)
    for _ in range(200):
        start_value = np.random.uniform(800000, 2000000)
        sequence = []
        for i in range(lookback):
            # T·∫°o decline rate realistic h∆°n
            decline_rate = np.random.uniform(0.01, 0.10)  # Gi·∫£m t·ª´ 2-20% xu·ªëng 1-10%
            if i == 0:
                sequence.append(start_value)
            else:
                sequence.append(sequence[-1] * (1 - decline_rate))
        
        target = sequence[-1] * (1 - np.random.uniform(0.01, 0.10))
        
        all_sequences.append(np.array(sequence).reshape(-1, 1))
        all_targets.append(target)
        trend_labels.append("decreasing")
        target_dates.append(pd.NaT)
        target_store_ids.append(None)
    
    # Volatile trends (GI·∫¢M T·ª™ 500 XU·ªêNG 100)
    for _ in range(100):
        start_value = np.random.uniform(800000, 1500000)
        sequence = []
        for i in range(lookback):
            if i == 0:
                sequence.append(start_value)
            else:
                # Gi·∫£m volatility
                change_rate = np.random.uniform(-0.08, 0.08)  # Gi·∫£m t·ª´ ¬±15% xu·ªëng ¬±8%
                sequence.append(sequence[-1] * (1 + change_rate))
        
        # Target based on recent trend
        recent_trend = (sequence[-1] - sequence[-3]) / sequence[-3]
        target = sequence[-1] * (1 + recent_trend * np.random.uniform(0.3, 0.8))  # Gi·∫£m multiplier
        
        all_sequences.append(np.array(sequence).reshape(-1, 1))
        all_targets.append(target)
        trend_labels.append("volatile")
        target_dates.append(pd.NaT)
        target_store_ids.append(None)
    
    # Stable trends (GI·∫¢M T·ª™ 500 XU·ªêNG 100)
    for _ in range(100):
        base_value = np.random.uniform(800000, 1500000)
        sequence = []
        for i in range(lookback):
            # Gi·∫£m variation
            variation = np.random.uniform(-0.03, 0.03)  # Gi·∫£m t·ª´ ¬±5% xu·ªëng ¬±3%
            sequence.append(base_value * (1 + variation))
        
        target = base_value * (1 + np.random.uniform(-0.03, 0.03))
        
        all_sequences.append(np.array(sequence).reshape(-1, 1))
        all_targets.append(target)
        trend_labels.append("stable")
        target_dates.append(pd.NaT)
        target_store_ids.append(None)
    
    # Convert to numpy arrays
    all_sequences = np.array(all_sequences)
    all_targets = np.array(all_targets)
    
    print(f"‚úÖ T·∫°o ƒë∆∞·ª£c {len(all_sequences):,} sequences")
    print(f"   ‚Ä¢ Increasing: {trend_labels.count('increasing'):,}")
    print(f"   ‚Ä¢ Decreasing: {trend_labels.count('decreasing'):,}")
    print(f"   ‚Ä¢ Volatile: {trend_labels.count('volatile'):,}")
    print(f"   ‚Ä¢ Stable: {trend_labels.count('stable'):,}")
    
    return all_sequences, all_targets, trend_labels, pd.to_datetime(target_dates), pd.Series(target_store_ids)

# ========== 3. ENHANCED TRAINING FUNCTION ==========
def train_improved_gru_model(model, train_loader, val_loader, num_epochs=150, learning_rate=0.001):
    """
    Hu·∫•n luy·ªán improved GRU model v·ªõi enhanced techniques
    """
    print(f"\nüöÄ B·∫Øt ƒë·∫ßu training Improved GRU model...")
    print(f"üìä S·ªë epochs: {num_epochs}")
    print(f"üìö Learning rate: {learning_rate}")
    
    # Loss function v√† optimizer
    criterion = nn.MSELoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', patience=15, factor=0.5, min_lr=1e-6
    )
    
    # L∆∞u training history
    train_losses = []
    val_losses = []
    best_val_loss = float('inf')
    patience_counter = 0
    
    for epoch in range(num_epochs):
        # Training phase
        model.train()
        train_loss = 0.0
        for sequences, targets in train_loader:
            sequences = sequences.to(device)
            targets = targets.to(device)
            
            optimizer.zero_grad()
            outputs = model(sequences)
            loss = criterion(outputs.squeeze(), targets)
            loss.backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            train_loss += loss.item()
        
        # Validation phase
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for sequences, targets in val_loader:
                sequences = sequences.to(device)
                targets = targets.to(device)
                
                outputs = model(sequences)
                loss = criterion(outputs.squeeze(), targets)
                val_loss += loss.item()
        
        # T√≠nh average loss
        train_loss /= len(train_loader)
        val_loss /= len(val_loader)
        
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        
        # Learning rate scheduling
        scheduler.step(val_loss)
        
        # Early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            # L∆∞u best model
            torch.save(model.state_dict(), 'improved_gru_model.pth')
        else:
            patience_counter += 1
        
        # Print progress
        if (epoch + 1) % 10 == 0:
            print(f"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}")
        
        # Early stopping (TƒÇNG PATIENCE)
        if patience_counter >= 50:  # TƒÉng t·ª´ 25 l√™n 50
            print(f"‚èπÔ∏è Early stopping t·∫°i epoch {epoch+1}")
            break
    
    # Load best model
    model.load_state_dict(torch.load('improved_gru_model.pth'))
    
    return model, train_losses, val_losses

# ========== 4. TREND VALIDATION FUNCTION ==========
def validate_trend_prediction(input_sequence, prediction, threshold=0.1):
    """
    Validate prediction d·ª±a tr√™n xu h∆∞·ªõng input
    """
    # T√≠nh xu h∆∞·ªõng c·ªßa input
    recent_trend = (input_sequence[-1] - input_sequence[-3]) / input_sequence[-3]
    
    # N·∫øu xu h∆∞·ªõng r√µ r√†ng (>10% change)
    if abs(recent_trend) > threshold:
        expected_direction = 1 if recent_trend > 0 else -1
        actual_direction = 1 if prediction > input_sequence[-1] else -1
        
        # N·∫øu prediction ng∆∞·ª£c v·ªõi xu h∆∞·ªõng
        if expected_direction != actual_direction:
            # Adjust prediction
            if expected_direction > 0:  # Increasing trend
                adjusted_prediction = input_sequence[-1] * (1 + abs(recent_trend) * 0.8)
            else:  # Decreasing trend
                adjusted_prediction = input_sequence[-1] * (1 - abs(recent_trend) * 0.8)
            
            return adjusted_prediction, True  # True = adjusted
    
    return prediction, False

# ========== 5. MAIN EXECUTION ==========
if __name__ == "__main__":
    print("="*60)
    print("üß† IMPROVED GRU SALES PREDICTION MODEL")
    print("="*60)
    
    # Parameters
    LOOKBACK = 10
    TRAIN_RATIO = 0.8
    VAL_RATIO = 0.1
    TEST_RATIO = 0.1
    BATCH_SIZE = 32  # Gi·∫£m batch size ƒë·ªÉ h·ªçc t·ªët h∆°n
    NUM_EPOCHS = 200  # TƒÉng epochs
    LEARNING_RATE = 0.0005  # Gi·∫£m learning rate ƒë·ªÉ h·ªçc ch·∫≠m h∆°n
    
    print(f"\nüìã PARAMETERS:")
    print(f"   ‚Ä¢ Lookback: {LOOKBACK} tu·∫ßn")
    print(f"   ‚Ä¢ Train/Val/Test: {TRAIN_RATIO}/{VAL_RATIO}/{TEST_RATIO}")
    print(f"   ‚Ä¢ Batch size: {BATCH_SIZE}")
    print(f"   ‚Ä¢ Epochs: {NUM_EPOCHS}")
    print(f"   ‚Ä¢ Learning rate: {LEARNING_RATE}")
    
    # 1. Load data
    print("\nüìä ƒêang ƒë·ªçc d·ªØ li·ªáu Walmart...")
    df = pd.read_csv("../walmart_processed_by_week.csv")
    df['Date'] = pd.to_datetime(df['Date'])
    
    # 2. T·∫°o balanced dataset
    sequences, targets, trend_labels, target_dates, target_store_ids = create_balanced_sequential_data(df, LOOKBACK)
    
    # 3. Split data (REAL-ONLY for val/test; add synthetic to train)
    is_real = ~pd.isna(target_dates)
    real_indices = np.where(is_real.values if hasattr(is_real, 'values') else is_real)[0]
    synthetic_indices = np.where(~(is_real.values if hasattr(is_real, 'values') else is_real))[0]

    num_real = len(real_indices)
    total_size = len(sequences)

    real_train_size = int(TRAIN_RATIO * num_real)
    real_val_size = int(VAL_RATIO * num_real)
    real_test_size = num_real - real_train_size - real_val_size

    # Gi·ªØ th·ª© t·ª± th·ªùi gian theo c√°ch ƒë√£ x√¢y d·ª±ng (cu·ªëi = m·ªõi nh·∫•t)
    train_real_idx = real_indices[:real_train_size]
    val_real_idx = real_indices[real_train_size:real_train_size + real_val_size]
    test_real_idx = real_indices[real_train_size + real_val_size:]

    # Train = real train + to√†n b·ªô synthetic (tƒÉng d·ªØ li·ªáu hu·∫•n luy·ªán)
    train_idx = np.concatenate([train_real_idx, synthetic_indices]) if len(synthetic_indices) > 0 else train_real_idx

    train_sequences = sequences[train_idx]
    train_targets = targets[train_idx]
    val_sequences = sequences[val_real_idx]
    val_targets = targets[val_real_idx]
    test_sequences = sequences[test_real_idx]
    test_targets = targets[test_real_idx]

    print(f"\nüìä DATA SPLIT (REAL/SYNTHETIC):")
    print(f"   ‚Ä¢ Total sequences: {total_size:,} (real: {num_real:,}, synthetic: {total_size - num_real:,})")
    print(f"   ‚Ä¢ Train: {len(train_sequences):,} samples (real: {len(train_real_idx):,}, synthetic: {len(synthetic_indices):,})")
    print(f"   ‚Ä¢ Validation (real only): {len(val_sequences):,} samples")
    print(f"   ‚Ä¢ Test (real only): {len(test_sequences):,} samples")
    
    # 4. Scale data
    print(f"\nüîß Scaling data...")
    
    # Scale sequences
    sequence_scaler = MinMaxScaler()
    train_sequences_scaled = sequence_scaler.fit_transform(train_sequences.reshape(-1, train_sequences.shape[-1])).reshape(train_sequences.shape)
    val_sequences_scaled = sequence_scaler.transform(val_sequences.reshape(-1, val_sequences.shape[-1])).reshape(val_sequences.shape)
    test_sequences_scaled = sequence_scaler.transform(test_sequences.reshape(-1, test_sequences.shape[-1])).reshape(test_sequences.shape)
    
    # Scale targets
    target_scaler = MinMaxScaler()
    train_targets_scaled = target_scaler.fit_transform(train_targets.reshape(-1, 1)).flatten()
    val_targets_scaled = target_scaler.transform(val_targets.reshape(-1, 1)).flatten()
    test_targets_scaled = target_scaler.transform(test_targets.reshape(-1, 1)).flatten()
    
    # 5. Create datasets v√† dataloaders
    class WalmartSequentialDataset(Dataset):
        def __init__(self, sequences, targets):
            self.sequences = sequences
            self.targets = targets
        
        def __len__(self):
            return len(self.sequences)
        
        def __getitem__(self, idx):
            return torch.tensor(self.sequences[idx], dtype=torch.float32), torch.tensor(self.targets[idx], dtype=torch.float32)
    
    train_dataset = WalmartSequentialDataset(train_sequences_scaled, train_targets_scaled)
    val_dataset = WalmartSequentialDataset(val_sequences_scaled, val_targets_scaled)
    test_dataset = WalmartSequentialDataset(test_sequences_scaled, test_targets_scaled)
    
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)
    
    # 6. Create v√† train improved model (T·ªêI ∆ØU ARCHITECTURE)
    input_size = 1
    model = ImprovedGRUSalesPredictor(
        input_size=input_size, 
        hidden_size=128,  # Gi·∫£m t·ª´ 256 xu·ªëng 128
        num_layers=2,     # Gi·∫£m t·ª´ 3 xu·ªëng 2
        dropout=0.2       # Gi·∫£m t·ª´ 0.3 xu·ªëng 0.2
    )
    model = model.to(device)
    
    print(f"\nüèóÔ∏è OPTIMIZED MODEL ARCHITECTURE:")
    print(f"   ‚Ä¢ Input size: {input_size}")
    print(f"   ‚Ä¢ Hidden size: 128")
    print(f"   ‚Ä¢ Num layers: 2")
    print(f"   ‚Ä¢ Dropout: 0.2")
    print(f"   ‚Ä¢ Bidirectional: True")
    print(f"   ‚Ä¢ Attention: True")
    print(f"   ‚Ä¢ Synthetic data: Reduced (600 samples)")
    print(f"   ‚Ä¢ Training: Longer (200 epochs, patience=50)")
    
    # Train model
    model, train_losses, val_losses = train_improved_gru_model(
        model, train_loader, val_loader, 
        num_epochs=NUM_EPOCHS, learning_rate=LEARNING_RATE
    )
    
    # 7. Evaluate model
    print(f"\nüìä EVALUATING IMPROVED MODEL...")
    
    def evaluate_improved_model(model, test_loader, scaler):
        model.eval()
        predictions = []
        actuals = []
        
        with torch.no_grad():
            for sequences, targets in test_loader:
                sequences = sequences.to(device)
                outputs = model(sequences)
                predictions.extend(outputs.squeeze().cpu().numpy())
                actuals.extend(targets.numpy())
        
        # Convert to numpy arrays
        predictions = np.array(predictions)
        actuals = np.array(actuals)
        
        # Inverse transform
        predictions = scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()
        actuals = scaler.inverse_transform(actuals.reshape(-1, 1)).flatten()
        
        # Calculate metrics
        rmse = np.sqrt(mean_squared_error(actuals, predictions))
        mae = mean_absolute_error(actuals, predictions)
        r2 = r2_score(actuals, predictions)
        
        return predictions, actuals, rmse, mae, r2
    
    predictions, actuals, rmse, mae, r2 = evaluate_improved_model(model, test_loader, target_scaler)
    
    print(f"\nüèÜ IMPROVED MODEL RESULTS:")
    print(f"   ‚Ä¢ RMSE: ${rmse:,.2f}")
    print(f"   ‚Ä¢ MAE: ${mae:,.2f}")
    print(f"   ‚Ä¢ R¬≤ Score: {r2:.4f}")
    
    # 8. Save improved model v√† scalers
    import pickle
    with open('improved_sequence_scaler.pkl', 'wb') as f:
        pickle.dump(sequence_scaler, f)
    with open('improved_target_scaler.pkl', 'wb') as f:
        pickle.dump(target_scaler, f)
    
    print(f"\n‚úÖ IMPROVED MODEL TRAINING COMPLETED!")
    print(f"   ‚Ä¢ Improved model saved: improved_gru_model.pth")
    print(f"   ‚Ä¢ Improved scalers saved: improved_sequence_scaler.pkl, improved_target_scaler.pkl")
    
    # 9. Test trend validation
    print(f"\nüß™ TESTING TREND VALIDATION...")
    
    # Test decreasing trend
    decreasing_sequence = [1450000, 1400000, 1350000, 1300000, 1250000, 
                          1200000, 1150000, 1100000, 1000000, 900000]
    
    # Scale sequence
    sequence_scaled = sequence_scaler.transform(np.array(decreasing_sequence).reshape(-1, 1)).reshape(1, -1, 1)
    sequence_tensor = torch.tensor(sequence_scaled, dtype=torch.float32).to(device)
    
    # Predict
    with torch.no_grad():
        prediction_scaled = model(sequence_tensor)
        prediction = target_scaler.inverse_transform(prediction_scaled.cpu().numpy().reshape(-1, 1))[0, 0]
    
    # Validate trend
    adjusted_prediction, was_adjusted = validate_trend_prediction(decreasing_sequence, prediction)
    
    print(f"üìä Decreasing Trend Test:")
    print(f"   ‚Ä¢ Input: {decreasing_sequence[-3:]}...")
    print(f"   ‚Ä¢ Raw Prediction: ${prediction:,.2f}")
    print(f"   ‚Ä¢ Adjusted Prediction: ${adjusted_prediction:,.2f}")
    print(f"   ‚Ä¢ Was Adjusted: {was_adjusted}")
    
    print(f"\nüìã OPTIMIZED SUMMARY:")
    print(f"   ‚Ä¢ Optimized GRU model ƒë√£ ƒë∆∞·ª£c train th√†nh c√¥ng")
    print(f"   ‚Ä¢ Reduced synthetic data (600 samples thay v√¨ 3000)")
    print(f"   ‚Ä¢ Simplified architecture (128 hidden, 2 layers)")
    print(f"   ‚Ä¢ Longer training (200 epochs, patience=50)")
    print(f"   ‚Ä¢ Better learning rate (0.0005)")
    print(f"   ‚Ä¢ Test R¬≤: {r2:.4f}")
    print(f"   ‚Ä¢ Expected improvement: Higher R¬≤ score")
    # Xu·∫•t ƒë√∫ng c√°c d√≤ng TEST th·ª±c t·∫ø (kh√¥ng synthetic) v·ªõi c·ªôt date + gru_pred, 1 d√≤ng/sequence test
    test_target_dates = pd.to_datetime(target_dates[test_real_idx])
    df_gru_test = pd.DataFrame({'date': test_target_dates, 'gru_pred': predictions})
    # Kh√¥ng g·ªôp theo date ƒë·ªÉ gi·ªØ ch√≠nh x√°c s·ªë samples test (v√≠ d·ª• 599)
    out_path = r'E:\TrainAI\Train\Walmart_new\gru_predictions.csv'
    df_gru_test.to_csv(out_path, index=False)
    print(f"‚úÖ ƒê√£ l∆∞u {len(df_gru_test)} d√≤ng test v·ªõi gru_pred v√†o: {out_path}")

    # 11. G·∫Øn c·ªôt d·ª± ƒëo√°n v√†o walmart_processed_by_week.csv (nh·ªØng d√≤ng kh√¥ng ph·∫£i test = NaN)
    df_with_pred = df.copy()
    df_with_pred['Date'] = pd.to_datetime(df_with_pred['Date'])
    # L·∫•y store cho t·ª´ng prediction test
    test_store_ids = target_store_ids[test_real_idx]
    df_test_pred = pd.DataFrame({
        'Date': test_target_dates,
        'Store': test_store_ids.values,
        'gru_pred': predictions
    })
    # Merge theo (Store, Date) ƒë·ªÉ ƒë√∫ng t·ª´ng c·ª≠a h√†ng/tu·∫ßn
    df_with_pred = df_with_pred.merge(df_test_pred, on=['Store', 'Date'], how='left')
    out_walmart_with_pred = r'E:\TrainAI\Train\walmart_processed_by_week_with_gru_pred.csv'
    df_with_pred.to_csv(out_walmart_with_pred, index=False)
    print(f"‚úÖ ƒê√£ l∆∞u file ƒë·∫ßy ƒë·ªß v·ªõi c·ªôt gru_pred (NaN ·ªü c√°c d√≤ng kh√¥ng test): {out_walmart_with_pred}")