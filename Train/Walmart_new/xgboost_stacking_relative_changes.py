import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.inspection import permutation_importance
import matplotlib.pyplot as plt
import seaborn as sns
import shap
import os
import warnings
import joblib
from datetime import datetime
import traceback
warnings.filterwarnings('ignore')

# Cáº¥u hÃ¬nh hiá»ƒn thá»‹
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)

# Thiáº¿t láº­p font cho matplotlib
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

def load_and_prepare_data():
    """
    Load dá»¯ liá»‡u Walmart vÃ  GRU predictions cho XGBoost Adjustment model
    """
    print("ğŸ”„ Äang load dá»¯ liá»‡u cho XGBoost Adjustment model...")
    
    # Thá»­ Ä‘á»c file Ä‘Ã£ merge
    walmart_path = "E:/TrainAI/Train/walmart_processed_by_week_with_gru_pred.csv"
    try:
        df = pd.read_csv(walmart_path)
        print(f"âœ… ÄÃ£ load file: {walmart_path}")
        print(f"ğŸ“Š Shape: {df.shape}")
        print(f"ğŸ“… Date range: {df['Date'].min()} Ä‘áº¿n {df['Date'].max()}")
        
        # Kiá»ƒm tra cá»™t GRU predictions
        if 'gru_pred' in df.columns:
            print(f"âœ… TÃ¬m tháº¥y cá»™t GRU predictions")
            print(f"   GRU predictions range: {df['gru_pred'].min():.2f} Ä‘áº¿n {df['gru_pred'].max():.2f}")
        else:
            print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y cá»™t GRU predictions")
            
    except FileNotFoundError:
        print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y file merged, Ä‘ang táº¡o fallback...")
        
        # Fallback: Ä‘á»c file gá»‘c vÃ  táº¡o dummy GRU predictions
        original_path = "E:/TrainAI/Train/walmart_processed_by_week.csv"
        try:
            df = pd.read_csv(original_path)
            print(f"âœ… ÄÃ£ load file gá»‘c: {original_path}")
            
            # Táº¡o dummy GRU predictions (naive approach)
            df['gru_pred'] = df['Weekly_Sales'] * 0.95 + np.random.normal(0, 100, len(df))
            print("âš ï¸ ÄÃ£ táº¡o dummy GRU predictions")
        except FileNotFoundError:
            print("âŒ KhÃ´ng tÃ¬m tháº¥y file nÃ o, vui lÃ²ng kiá»ƒm tra Ä‘Æ°á»ng dáº«n")
            return None
    
    # Kiá»ƒm tra dá»¯ liá»‡u
    print(f"ğŸ“‹ Columns: {list(df.columns)}")
    print(f"ğŸ” Missing values:\n{df.isnull().sum()}")
    
    # Kiá»ƒm tra Weekly_Sales vÃ  GRU predictions
    if 'Weekly_Sales' in df.columns and 'gru_pred' in df.columns:
        print(f"ğŸ¯ Target adjustment sáº½ lÃ : Weekly_Sales - GRU_pred")
        adjustments = df['Weekly_Sales'] - df['gru_pred']
        print(f"   Adjustment range: {adjustments.min():.2f} Ä‘áº¿n {adjustments.max():.2f}")
        print(f"   Adjustment mean: {adjustments.mean():.2f}")
    
    return df

def prepare_features_absolute(df):
    """
    Chuáº©n bá»‹ features cho XGBoost Adjustment model: sá»­ dá»¥ng giÃ¡ trá»‹ tuyá»‡t Ä‘á»‘i cá»§a tuáº§n dá»± Ä‘oÃ¡n
    """
    print("ğŸ”„ Äang chuáº©n bá»‹ features cho Adjustment model (Absolute values)...")
    df['Date'] = pd.to_datetime(df['Date'])
    df['DayOfWeek'] = df['Date'].dt.dayofweek
    df['Is_Weekend'] = (df['DayOfWeek'] >= 5).astype(int)
    print("âœ… ÄÃ£ chuáº©n bá»‹ xong features cho Adjustment model (Absolute values)")
    return df

def prepare_features_relative(df):
    """
    Chuáº©n bá»‹ features cho XGBoost Adjustment model: sá»­ dá»¥ng thay Ä‘á»•i tÆ°Æ¡ng Ä‘á»‘i so vá»›i tuáº§n trÆ°á»›c
    """
    print("ğŸ”„ Äang chuáº©n bá»‹ features cho Adjustment model (Relative changes)...")
    df['Date'] = pd.to_datetime(df['Date'])
    
    # TÃ­nh thay Ä‘á»•i tÆ°Æ¡ng Ä‘á»‘i so vá»›i tuáº§n trÆ°á»›c
    df['Temperature_change'] = df['Temperature'].pct_change() * 100  # %
    df['Fuel_Price_change'] = df['Fuel_Price'].pct_change() * 100   # %
    df['CPI_change'] = df['CPI'].pct_change() * 100                 # %
    df['Unemployment_change'] = df['Unemployment'].pct_change() * 100 # %
    
    # Xá»­ lÃ½ NaN values tá»« pct_change() (hÃ ng Ä‘áº§u tiÃªn)
    print("âš ï¸ Äang xá»­ lÃ½ NaN values tá»« relative changes...")
    df['Temperature_change'] = df['Temperature_change'].fillna(0)  # KhÃ´ng thay Ä‘á»•i
    df['Fuel_Price_change'] = df['Fuel_Price_change'].fillna(0)    # KhÃ´ng thay Ä‘á»•i
    df['CPI_change'] = df['CPI_change'].fillna(0)                  # KhÃ´ng thay Ä‘á»•i
    df['Unemployment_change'] = df['Unemployment_change'].fillna(0) # KhÃ´ng thay Ä‘á»•i
    
    # Giá»¯ nguyÃªn cÃ¡c features khÃ¡c
    df['DayOfWeek'] = df['Date'].dt.dayofweek
    df['Is_Weekend'] = (df['DayOfWeek'] >= 5).astype(int)
    
    print("âœ… ÄÃ£ chuáº©n bá»‹ xong features cho Adjustment model (Relative changes)")
    return df

def prepare_features_and_target(df, feature_type='absolute'):
    """
    Chuáº©n bá»‹ features vÃ  target cho training
    XGBoost sáº½ há»c cÃ¡ch Ä‘iá»u chá»‰nh GRU predictions theo tá»· lá»‡, khÃ´ng pháº£i giÃ¡ trá»‹ tuyá»‡t Ä‘á»‘i
    """
    print(f"ğŸ”„ Äang chuáº©n bá»‹ features vÃ  target ({feature_type})...")
    
    # Kiá»ƒm tra GRU predictions
    if 'gru_pred' not in df.columns:
        print("âŒ KhÃ´ng tÃ¬m tháº¥y cá»™t gru_pred")
        return None, None, None
    
    # Lá»c ra cÃ¡c hÃ ng cÃ³ GRU predictions (test set)
    df_with_gru = df.dropna(subset=['gru_pred']).copy()
    print(f"ğŸ“Š Sá»‘ máº«u cÃ³ GRU predictions: {len(df_with_gru)}")
    
    if len(df_with_gru) == 0:
        print("âŒ KhÃ´ng cÃ³ máº«u nÃ o cÃ³ GRU predictions")
        return None, None, None
    
    # Chuáº©n bá»‹ features dá»±a trÃªn feature_type
    if feature_type == 'absolute':
        df_with_gru = prepare_features_absolute(df_with_gru)
    else:  # relative
        df_with_gru = prepare_features_relative(df_with_gru)
    
    # Chá»n features cho XGBoost (LOáº I Bá» gru_pred vÃ¬ sáº½ dÃ¹ng Ä‘á»ƒ tÃ­nh target)
    if feature_type == 'absolute':
        candidate_features = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 
                           'Holiday_Flag', 'Month', 'WeekOfYear', 'Year', 'DayOfWeek', 'Is_Weekend']
    else:  # relative
        candidate_features = ['Temperature_change', 'Fuel_Price_change', 'CPI_change', 'Unemployment_change',
                           'Holiday_Flag', 'Month', 'WeekOfYear', 'Year', 'DayOfWeek', 'Is_Weekend']
    
    # Kiá»ƒm tra features cÃ³ sáºµn
    available_features = [col for col in candidate_features if col in df_with_gru.columns]
    print(f"âœ… Features cÃ³ sáºµn: {available_features}")
    
    # Chuáº©n bá»‹ X (features) vÃ  y (target)
    X = df_with_gru[available_features].copy()
    
    # ğŸš€ TARGET Má»šI: ÄIá»€U CHá»ˆNH TÆ¯Æ NG Äá»I thay vÃ¬ tuyá»‡t Ä‘á»‘i
    # Thay vÃ¬: adjustment = Weekly_Sales_thá»±c_táº¿ - GRU_prediction
    # BÃ¢y giá»: adjustment_ratio = (Weekly_Sales_thá»±c_táº¿ - GRU_prediction) / GRU_prediction
    # XGBoost sáº½ há»c cÃ¡ch dá»± Ä‘oÃ¡n adjustment_ratio nÃ y
    y = (df_with_gru['Weekly_Sales'] - df_with_gru['gru_pred']) / df_with_gru['gru_pred']
    
    print(f"ğŸ¯ Target Má»šI: XGBoost há»c cÃ¡ch dá»± Ä‘oÃ¡n ADJUSTMENT RATIO")
    print(f"   Adjustment ratio = (Weekly_Sales - GRU_pred) / GRU_pred")
    print(f"   Adjustment ratio range: {y.min():.4f} Ä‘áº¿n {y.max():.4f}")
    print(f"   Adjustment ratio mean: {y.mean():.4f}")
    print(f"   VÃ­ dá»¥: 0.05 = tÄƒng 5%, -0.03 = giáº£m 3%")
    
    # Kiá»ƒm tra NaN vÃ  infinity
    print("ğŸ” Kiá»ƒm tra dá»¯ liá»‡u...")
    print(f"Target NaN: {y.isnull().sum()}")
    print(f"Target Infinity: {np.isinf(y).sum()}")
    print(f"Features NaN: {X.isnull().sum().sum()}")
    
    # Xá»­ lÃ½ NaN vÃ  infinity trong target
    if y.isnull().sum() > 0 or np.isinf(y).sum() > 0:
        print("âš ï¸ Äang xá»­ lÃ½ NaN vÃ  Infinity trong target...")
        # Thay tháº¿ NaN vÃ  Infinity báº±ng 0 (khÃ´ng Ä‘iá»u chá»‰nh)
        y = y.replace([np.inf, -np.inf], 0).fillna(0)
        print(f"   Sau khi xá»­ lÃ½: NaN: {y.isnull().sum()}, Infinity: {np.isinf(y).sum()}")
    
    # Xá»­ lÃ½ NaN trong features
    if X.isnull().sum().sum() > 0:
        print("âš ï¸ Äang xá»­ lÃ½ NaN trong features...")
        X = X.fillna(X.mean())
    
    # Kiá»ƒm tra cuá»‘i cÃ¹ng
    if y.isnull().sum() > 0 or X.isnull().sum().sum() > 0:
        print("âŒ Váº«n cÃ²n NaN values sau khi xá»­ lÃ½")
        return None, None, None
    
    print(f"âœ… Features shape: {X.shape}")
    print(f"âœ… Target shape: {y.shape}")
    print(f"âœ… Target range: {y.min():.4f} Ä‘áº¿n {y.max():.4f}")
    
    return X, y, available_features

def train_xgboost_model(X, y):
    """
    Train XGBoost model Ä‘á»ƒ dá»± Ä‘oÃ¡n ADJUSTMENT cho GRU predictions
    """
    print("ğŸ”„ Äang train XGBoost Adjustment model...")
    
    # Split data vá»›i shuffle=False Ä‘á»ƒ giá»¯ thá»© tá»± thá»i gian
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, shuffle=False, random_state=42
    )
    
    print(f"ğŸ“Š Train set: {X_train.shape[0]} samples")
    print(f"ğŸ“Š Test set: {X_test.shape[0]} samples")
    print(f"ğŸ¯ Target: XGBoost há»c cÃ¡ch dá»± Ä‘oÃ¡n ADJUSTMENT")
    print(f"   Adjustment range: {y.min():.2f} Ä‘áº¿n {y.max():.2f}")
    
    # XGBoost parameters - tá»‘i Æ°u hÃ³a Ä‘á»ƒ trÃ¡nh overfitting
    params = {
        'n_estimators': 200,          # Giáº£m tá»« 500
        'learning_rate': 0.03,        # Giáº£m tá»« 0.05
        'max_depth': 4,               # Giáº£m tá»« 5
        'min_child_weight': 3,        # TÄƒng regularization
        'subsample': 0.8,             # ThÃªm regularization
        'colsample_bytree': 0.8,      # ThÃªm regularization
        'reg_alpha': 0.1,             # L1 regularization
        'reg_lambda': 1.0,            # L2 regularization
        'eval_metric': 'rmse',        # ThÃªm eval_metric
        'random_state': 42
    }
    
    print(f"âš™ï¸ XGBoost parameters: {params}")
    
    # Train model
    model = xgb.XGBRegressor(**params)
    
    # Early stopping Ä‘á»ƒ trÃ¡nh overfitting
    eval_set = [(X_train, y_train), (X_test, y_test)]
    model.fit(X_train, y_train, eval_set=eval_set, verbose=False)
    
    print("âœ… ÄÃ£ train xong XGBoost Adjustment model")
    
    return model, X_train, X_test, y_train, y_test

def evaluate_model(model, X_train, X_test, y_train, y_test, feature_type, df_test=None):
    """
    ÄÃ¡nh giÃ¡ model vÃ  tÃ­nh ensemble predictions
    XGBoost giá» Ä‘Ã¢y dá»± Ä‘oÃ¡n ADJUSTMENT RATIO, khÃ´ng pháº£i Weekly_Sales trá»±c tiáº¿p
    """
    print(f"ğŸ”„ Äang Ä‘Ã¡nh giÃ¡ model ({feature_type})...")
    
    # Predictions - XGBoost dá»± Ä‘oÃ¡n ADJUSTMENT RATIO
    adjustment_ratio_train_pred = model.predict(X_train)
    adjustment_ratio_test_pred = model.predict(X_test)
    
    print("ğŸ¯ XGBoost dá»± Ä‘oÃ¡n ADJUSTMENT RATIO ((Weekly_Sales - GRU_pred) / GRU_pred)")
    print(f"   Adjustment ratio range: {adjustment_ratio_test_pred.min():.4f} Ä‘áº¿n {adjustment_ratio_test_pred.max():.4f}")
    print(f"   VÃ­ dá»¥: 0.05 = tÄƒng 5%, -0.03 = giáº£m 3%")
    
    # TÃ­nh ensemble predictions (GRU + XGBoost adjustment)
    print("ğŸ”„ Äang tÃ­nh ensemble predictions (GRU + XGBoost adjustment ratio)...")
    
    # Láº¥y GRU predictions tá»« test set
    # ChÃºng ta cáº§n df_test Ä‘á»ƒ truy cáº­p gru_pred
    if df_test is not None and 'gru_pred' in df_test.columns:
        gru_pred_test = df_test['gru_pred'].iloc[X_test.index]
        print(f"âœ… ÄÃ£ láº¥y GRU predictions tá»« df_test")
    else:
        print("âš ï¸ KhÃ´ng cÃ³ df_test, sá»­ dá»¥ng fallback logic")
        # Fallback: giáº£ sá»­ y_test lÃ  adjustment ratio vÃ  tÃ­nh ngÆ°á»£c láº¡i
        # Äiá»u nÃ y khÃ´ng lÃ½ tÆ°á»Ÿng nhÆ°ng Ä‘á»ƒ trÃ¡nh lá»—i
        gru_pred_test = np.ones_like(adjustment_ratio_test_pred) * 1000000  # Placeholder 1M
    
    # ğŸš€ ENSEMBLE Má»šI: final_pred = GRU_pred * (1 + adjustment_ratio)
    # Thay vÃ¬: final_pred = GRU_pred + adjustment
    # BÃ¢y giá»: final_pred = GRU_pred * (1 + adjustment_ratio)
    ensemble_test_pred = gru_pred_test * (1 + adjustment_ratio_test_pred)
    
    print(f"ğŸ“Š Ensemble Má»šI: GRU_pred * (1 + adjustment_ratio)")
    print(f"   GRU predictions range: {gru_pred_test.min():.2f} Ä‘áº¿n {gru_pred_test.max():.2f}")
    print(f"   XGBoost adjustment ratios range: {adjustment_ratio_test_pred.min():.4f} Ä‘áº¿n {adjustment_ratio_test_pred.max():.4f}")
    print(f"   Final ensemble range: {ensemble_test_pred.min():.2f} Ä‘áº¿n {ensemble_test_pred.max():.2f}")
    
    # TÃ­nh metrics cho tá»«ng model
    print("\nğŸ“Š Káº¾T QUáº¢ ÄÃNH GIÃ:")
    print("=" * 50)
    if df_test is not None and 'Weekly_Sales' in df_test.columns:
        weekly_sales_test = df_test['Weekly_Sales'].iloc[X_test.index]
        print(f"âœ… ÄÃ£ láº¥y Weekly_Sales thá»±c táº¿ tá»« df_test")
    else:
        print("âš ï¸ KhÃ´ng cÃ³ df_test, sá»­ dá»¥ng fallback logic")
        # Fallback: tÃ­nh ngÆ°á»£c láº¡i tá»« adjustment ratio
        weekly_sales_test = gru_pred_test * (1 + y_test)
    
    gru_r2 = r2_score(weekly_sales_test, gru_pred_test)
    gru_rmse = np.sqrt(mean_squared_error(weekly_sales_test, gru_pred_test))
    gru_mae = mean_absolute_error(weekly_sales_test, gru_pred_test)
    
    print(f"ğŸ¯ GRU Model (Baseline):")
    print(f"   RÂ² Score: {gru_r2:.4f}")
    print(f"   RMSE: {gru_rmse:.2f}")
    print(f"   MAE: {gru_mae:.2f}")
    
    # 2. XGBoost model - dá»± Ä‘oÃ¡n ADJUSTMENT RATIO
    xgb_r2 = r2_score(y_test, adjustment_ratio_test_pred)  # y_test lÃ  adjustment ratio thá»±c táº¿
    xgb_rmse = np.sqrt(mean_squared_error(y_test, adjustment_ratio_test_pred))
    xgb_mae = mean_absolute_error(y_test, adjustment_ratio_test_pred)
    
    print(f"\nğŸŒ³ XGBoost Model (Adjustment Ratio):")
    print(f"   RÂ² Score: {xgb_r2:.4f}")
    print(f"   RMSE: {xgb_rmse:.4f}")
    print(f"   MAE: {xgb_mae:.4f}")
    
    # 3. Ensemble model: GRU_pred * (1 + XGBoost_adjustment_ratio)
    ensemble_r2 = r2_score(weekly_sales_test, ensemble_test_pred)
    ensemble_rmse = np.sqrt(mean_squared_error(weekly_sales_test, ensemble_test_pred))
    ensemble_mae = mean_absolute_error(weekly_sales_test, ensemble_test_pred)
    
    print(f"\nğŸš€ Ensemble Model (GRU * (1 + XGBoost Adjustment Ratio)):")
    print(f"   RÂ² Score: {ensemble_r2:.4f}")
    print(f"   RMSE: {ensemble_rmse:.2f}")
    print(f"   MAE: {ensemble_mae:.2f}")
    
    # So sÃ¡nh hiá»‡u suáº¥t
    print(f"\nğŸ“ˆ SO SÃNH HIá»†U SUáº¤T:")
    print(f"   Ensemble vs GRU:")
    print(f"     RÂ² improvement: {ensemble_r2 - gru_r2:.4f}")
    print(f"     RMSE improvement: {gru_rmse - ensemble_rmse:.2f}")
    print(f"     MAE improvement: {gru_mae - ensemble_mae:.2f}")
    
    # Kiá»ƒm tra overfitting
    train_r2 = r2_score(y_train, model.predict(X_train))
    r2_diff = train_r2 - xgb_r2
    
    if abs(r2_diff) < 0.1:
        print(f"\nâœ… Model á»•n Ä‘á»‹nh (RÂ² diff: {r2_diff:.4f})")
    else:
        print(f"\nâš ï¸ Model cÃ³ thá»ƒ bá»‹ overfitting (RÂ² diff: {r2_diff:.4f})")
    
    return {
        'gru': {'r2': gru_r2, 'rmse': gru_rmse, 'mae': gru_mae},
        'xgb': {'r2': xgb_r2, 'rmse': xgb_rmse, 'mae': xgb_mae},
        'ensemble': {'r2': ensemble_r2, 'rmse': ensemble_rmse, 'mae': ensemble_mae},
        'ensemble_predictions': ensemble_test_pred
    }

def calculate_feature_importance(model, X_test, y_test, available_features, feature_type):
    """
    TÃ­nh feature importance vá»›i nhiá»u phÆ°Æ¡ng phÃ¡p cho XGBoost Adjustment model
    """
    print(f"ğŸ”„ Äang tÃ­nh feature importance cho Adjustment model ({feature_type})...")
    
    # 1. Gain Importance (XGBoost)
    gain_importance = model.feature_importances_
    gain_df = pd.DataFrame({
        'Feature': available_features,
        'Gain_Importance': gain_importance
    }).sort_values('Gain_Importance', ascending=False)
    
    # 2. Permutation Importance
    perm_importance = permutation_importance(
        model, X_test, y_test, n_repeats=10, random_state=42
    )
    perm_df = pd.DataFrame({
        'Feature': available_features,
        'Permutation_Importance': perm_importance.importances_mean
    }).sort_values('Permutation_Importance', ascending=False)
    
    # 3. SHAP Values
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X_test)
    
    # TÃ­nh SHAP importance (mean absolute SHAP values)
    shap_importance = np.abs(shap_values).mean(axis=0)
    shap_df = pd.DataFrame({
        'Feature': available_features,
        'SHAP_Importance': shap_importance
    }).sort_values('SHAP_Importance', ascending=False)
    
    print("âœ… ÄÃ£ tÃ­nh xong feature importance cho Adjustment model")
    print(f"ğŸ’¡ Features quan trá»ng nháº¥t Ä‘á»ƒ dá»± Ä‘oÃ¡n ADJUSTMENT:")
    print(f"   Top 3 (Gain): {gain_df.head(3)['Feature'].tolist()}")
    
    return gain_df, perm_df, shap_df, shap_values, explainer

def create_visualizations(gain_df, perm_df, shap_df, shap_values, explainer,
                         X_test, y_test, ensemble_predictions, feature_type, available_features, gru_predictions=None):
    """
    Táº¡o cÃ¡c biá»ƒu Ä‘á»“ visualization
    """
    print(f"ğŸ¨ Äang táº¡o biá»ƒu Ä‘á»“ ({feature_type})...")
    
    # Táº¡o thÆ° má»¥c output
    output_dir = f"output_{feature_type}"
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. Feature Importance Comparison
    plt.figure(figsize=(15, 10))
    
    # Subplot 1: Gain Importance
    plt.subplot(2, 2, 1)
    top_gain = gain_df.head(10)
    plt.barh(range(len(top_gain)), top_gain['Gain_Importance'])
    plt.yticks(range(len(top_gain)), top_gain['Feature'])
    plt.xlabel('Gain Importance')
    plt.title('Top 10 Features - Gain Importance')
    plt.gca().invert_yaxis()
    
    # Subplot 2: Permutation Importance
    plt.subplot(2, 2, 2)
    top_perm = perm_df.head(10)
    plt.barh(range(len(top_perm)), top_perm['Permutation_Importance'])
    plt.yticks(range(len(top_perm)), top_perm['Feature'])
    plt.xlabel('Permutation Importance')
    plt.title('Top 10 Features - Permutation Importance')
    plt.gca().invert_yaxis()
    
    # Subplot 3: SHAP Importance
    plt.subplot(2, 2, 3)
    top_shap = shap_df.head(10)
    plt.barh(range(len(top_shap)), top_shap['SHAP_Importance'])
    plt.yticks(range(len(top_shap)), top_shap['SHAP_Importance'])
    plt.xlabel('SHAP Importance')
    plt.title('Top 10 Features - SHAP Importance')
    plt.gca().invert_yaxis()
    
    # Subplot 4: Feature Importance Comparison
    plt.subplot(2, 2, 4)
    comparison_df = pd.merge(gain_df, shap_df, on='Feature', suffixes=('_Gain', '_SHAP'))
    comparison_df = comparison_df.head(10)
    
    x = np.arange(len(comparison_df))
    width = 0.35
    
    plt.bar(x - width/2, comparison_df['Gain_Importance'], width, label='Gain', alpha=0.8)
    plt.bar(x + width/2, comparison_df['SHAP_Importance'], width, label='SHAP', alpha=0.8)
    plt.xlabel('Features')
    plt.ylabel('Importance')
    plt.title('Gain vs SHAP Importance')
    plt.xticks(x, comparison_df['Feature'], rotation=45, ha='right')
    plt.legend()
    plt.tight_layout()
    
    plt.savefig(f"{output_dir}/feature_importance_comparison.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 2. SHAP Summary Plot
    plt.figure(figsize=(12, 8))
    shap.summary_plot(shap_values, X_test, plot_type="bar", show=False)
    plt.title(f'SHAP Feature Importance - {feature_type.title()}')
    plt.tight_layout()
    plt.savefig(f"{output_dir}/shap_summary.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 3. SHAP Dependence Plots cho top 3 features
    top_features = gain_df.head(3)['Feature'].tolist()
    for i, feature in enumerate(top_features):
        if feature in X_test.columns:
            plt.figure(figsize=(10, 6))
            shap.dependence_plot(feature, shap_values, X_test, show=False)
            plt.title(f'SHAP Dependence Plot - {feature}')
            plt.tight_layout()
            plt.savefig(f"{output_dir}/shap_dependence_{feature}.png", dpi=300, bbox_inches='tight')
            plt.close()
    
    # 4. SHAP Force Plot
    plt.figure(figsize=(12, 8))
    shap.force_plot(explainer.expected_value, shap_values[0:100], X_test.iloc[0:100], show=False)
    plt.title(f'SHAP Force Plot - {feature_type.title()} (First 100 samples)')
    plt.tight_layout()
    plt.savefig(f"{output_dir}/shap_force.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 5. Ensemble vs GRU vs Actual Comparison
    plt.figure(figsize=(15, 10))
    
    # Láº¥y GRU predictions - sá»­ dá»¥ng parameter hoáº·c fallback
    if gru_predictions is not None:
        gru_pred_test = gru_predictions
        print("âœ… Sá»­ dá»¥ng GRU predictions tá»« parameter")
    else:
        print("âš ï¸ KhÃ´ng cÃ³ GRU predictions, bá» qua biá»ƒu Ä‘á»“ so sÃ¡nh")
        return
    
    # Subplot 1: Predictions vs Actual
    plt.subplot(2, 2, 1)
    plt.scatter(y_test, gru_pred_test, alpha=0.6, label='GRU Predictions', color='blue')
    plt.scatter(y_test, ensemble_predictions, alpha=0.6, label='Ensemble Predictions', color='red')
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
    plt.xlabel('Actual Weekly Sales')
    plt.ylabel('Predicted Weekly Sales')
    plt.title('Predictions vs Actual')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Subplot 2: Residuals comparison
    plt.subplot(2, 2, 2)
    gru_residuals = y_test - gru_pred_test
    ensemble_residuals = y_test - ensemble_predictions
    
    plt.scatter(gru_pred_test, gru_residuals, alpha=0.6, label='GRU Residuals', color='blue')
    plt.scatter(ensemble_predictions, ensemble_residuals, alpha=0.6, label='Ensemble Residuals', color='red')
    plt.axhline(y=0, color='k', linestyle='--', alpha=0.5)
    plt.xlabel('Predicted Values')
    plt.ylabel('Residuals')
    plt.title('Residuals Analysis')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Subplot 3: Time series comparison (first 50 samples)
    plt.subplot(2, 2, 3)
    sample_size = min(50, len(y_test))
    x_axis = range(sample_size)
    
    plt.plot(x_axis, y_test.iloc[:sample_size], 'k-', label='Actual', linewidth=2)
    plt.plot(x_axis, gru_pred_test.iloc[:sample_size], 'b--', label='GRU', linewidth=1.5)
    plt.plot(x_axis, ensemble_predictions[:sample_size], 'r--', label='Ensemble', linewidth=1.5)
    plt.xlabel('Sample Index')
    plt.ylabel('Weekly Sales')
    plt.title('Time Series Comparison (First 50 samples)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Subplot 4: Performance metrics comparison
    plt.subplot(2, 2, 4)
    models = ['GRU', 'Ensemble']
    r2_scores = [r2_score(y_test, gru_pred_test), r2_score(y_test, ensemble_predictions)]
    rmse_scores = [np.sqrt(mean_squared_error(y_test, gru_pred_test)), 
                   np.sqrt(mean_squared_error(y_test, ensemble_predictions))]
    
    x = np.arange(len(models))
    width = 0.35
    
    ax1 = plt.gca()
    ax2 = ax1.twinx()
    
    bars1 = ax1.bar(x - width/2, r2_scores, width, label='RÂ² Score', color='skyblue', alpha=0.7)
    bars2 = ax2.bar(x + width/2, rmse_scores, width, label='RMSE', color='lightcoral', alpha=0.7)
    
    ax1.set_xlabel('Models')
    ax1.set_ylabel('RÂ² Score', color='skyblue')
    ax2.set_ylabel('RMSE', color='lightcoral')
    ax1.set_title('Performance Comparison')
    ax1.set_xticks(x)
    ax1.set_xticklabels(models)
    
    # Add value labels on bars
    for bar in bars1:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{height:.3f}', ha='center', va='bottom', color='skyblue')
    
    for bar in bars2:
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height + 1000,
                f'{height:.0f}', ha='center', va='bottom', color='lightcoral')
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/ensemble_comparison.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"âœ… ÄÃ£ táº¡o xong biá»ƒu Ä‘á»“ trong thÆ° má»¥c {output_dir}/")

def save_results(gain_df, perm_df, shap_df, y_test, ensemble_predictions, feature_type):
    """
    LÆ°u káº¿t quáº£ vÃ o file
    """
    print(f"ğŸ’¾ Äang lÆ°u káº¿t quáº£ ({feature_type})...")
    
    # Táº¡o thÆ° má»¥c output
    output_dir = f"output_{feature_type}"
    os.makedirs(output_dir, exist_ok=True)
    
    # LÆ°u feature importance
    gain_df.to_csv(f"{output_dir}/feature_importance_gain.csv", index=False)
    perm_df.to_csv(f"{output_dir}/feature_importance_permutation.csv", index=False)
    shap_df.to_csv(f"{output_dir}/feature_importance_shap.csv", index=False)
    
    # LÆ°u predictions - y_test giá» lÃ  adjustment ratio target
    predictions_df = pd.DataFrame({
        'adjustment_ratio_target': y_test,  # Target thá»±c táº¿ (Weekly_Sales - GRU_pred) / GRU_pred
        'ensemble_prediction': ensemble_predictions
    })
    predictions_df.to_csv(f"{output_dir}/ensemble_predictions.csv", index=False)
    
    # LÆ°u thÃªm thÃ´ng tin vá» adjustment ratios
    adjustment_ratio_stats = {
        'Min': y_test.min(),
        'Max': y_test.max(),
        'Mean': y_test.mean(),
        'Std': y_test.std()
    }
    
    # LÆ°u stats vÃ o file riÃªng
    stats_df = pd.DataFrame([adjustment_ratio_stats])
    stats_df.to_csv(f"{output_dir}/adjustment_ratio_stats.csv", index=False)
    
    # LÆ°u adjustment ratio targets
    adjustment_ratio_targets_df = pd.DataFrame({
        'adjustment_ratio_target': y_test
    })
    adjustment_ratio_targets_df.to_csv(f"{output_dir}/adjustment_ratio_targets.csv", index=False)
    
    print(f"âœ… ÄÃ£ lÆ°u káº¿t quáº£ vÃ o thÆ° má»¥c: {output_dir}")
    print(f"ğŸ’¡ LÆ°u Ã½: y_test giá» lÃ  ADJUSTMENT RATIO target ((Weekly_Sales - GRU_pred) / GRU_pred)")
    print(f"   VÃ­ dá»¥: 0.05 = tÄƒng 5%, -0.03 = giáº£m 3%")

def print_summary(gain_df, perm_df, shap_df, feature_type):
    """
    In summary káº¿t quáº£
    """
    print(f"\nğŸ“‹ SUMMARY Káº¾T QUáº¢ ({feature_type.upper()}):")
    print("=" * 60)
    
    # Top features theo Gain Importance
    print(f"ğŸ† TOP 10 FEATURES (Gain Importance):")
    for i, (_, row) in enumerate(gain_df.head(10).iterrows(), 1):
        print(f"   {i:2d}. {row['Feature']:<20} | Importance: {row['Gain_Importance']:.4f}")
    
    # Top features theo SHAP Importance
    print(f"\nğŸ” TOP 10 FEATURES (SHAP Importance):")
    for i, (_, row) in enumerate(shap_df.head(10).iterrows(), 1):
        print(f"   {i:2d}. {row['Feature']:<20} | SHAP: {row['SHAP_Importance']:.4f}")
    
    # Top features theo Permutation Importance
    print(f"\nğŸ”„ TOP 10 FEATURES (Permutation Importance):")
    for i, (_, row) in enumerate(perm_df.head(10).iterrows(), 1):
        print(f"   {i:2d}. {row['Feature']:<20} | Permutation: {row['Permutation_Importance']:.4f}")
    
    print(f"\nğŸ“ Files Ä‘Ã£ táº¡o:")
    print(f"   - feature_importance_gain.csv")
    print(f"   - feature_importance_permutation.csv") 
    print(f"   - feature_importance_shap.csv")
    print(f"   - ensemble_predictions.csv")
    print(f"   - adjustment_ratio_stats.csv")
    print(f"   - adjustment_ratio_targets.csv")
    print(f"   - CÃ¡c biá»ƒu Ä‘á»“ PNG trong thÆ° má»¥c output_{feature_type}/")
    
    print(f"\nğŸ’¡ LÆ¯U Ã QUAN TRá»ŒNG:")
    print(f"   - Model nÃ y dá»± Ä‘oÃ¡n ADJUSTMENT RATIO cho GRU predictions")
    print(f"   - Target: (Weekly_Sales - GRU_pred) / GRU_pred")
    print(f"   - Final prediction = GRU_pred * (1 + adjustment_ratio)")
    print(f"   - VÃ­ dá»¥: adjustment_ratio = 0.05 â†’ tÄƒng 5%")
    print(f"   - VÃ­ dá»¥: adjustment_ratio = -0.03 â†’ giáº£m 3%")

def export_model(model, feature_columns, feature_type, performance_metrics):
    """
    Xuáº¥t model Ä‘Ã£ train Ä‘á»ƒ sá»­ dá»¥ng trong API
    """
    print(f"\nğŸ’¾ ÄANG XUáº¤T MODEL CHO {feature_type.upper()}...")
    
    # Táº¡o thÆ° má»¥c output
    output_dir = f'output_{feature_type}'
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. LÆ°u XGBoost model
    model_path = f'{output_dir}/xgb_model.pkl'
    joblib.dump(model, model_path)
    print(f"âœ… ÄÃ£ lÆ°u XGBoost model táº¡i: {model_path}")
    
    # 2. LÆ°u feature columns
    feature_path = f'{output_dir}/feature_columns.txt'
    with open(feature_path, 'w') as f:
        for col in feature_columns:
            f.write(f"{col}\n")
    print(f"âœ… ÄÃ£ lÆ°u feature columns táº¡i: {feature_path}")
    
    # 3. LÆ°u model info
    info_path = f'{output_dir}/model_info.txt'
    with open(info_path, 'w', encoding='utf-8') as f:
        f.write(f"WALMART SALES PREDICTION MODEL INFO\n")
        f.write(f"=" * 50 + "\n\n")
        f.write(f"NgÃ y xuáº¥t: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Loáº¡i model: XGBoost Adjustment Ratio cho GRU\n")
        f.write(f"Tá»‰ lá»‡ ensemble: GRU_pred * (1 + XGBoost_adjustment_ratio)\n")
        f.write(f"Feature type: {feature_type}\n\n")
        
        f.write(f"PERFORMANCE METRICS:\n")
        f.write(f"-" * 30 + "\n")
        f.write(f"GRU RÂ²: {performance_metrics['gru']['r2']:.4f}\n")
        f.write(f"XGBoost Adjustment Ratio RÂ²: {performance_metrics['xgb']['r2']:.4f}\n")
        f.write(f"Ensemble RÂ²: {performance_metrics['ensemble']['r2']:.4f}\n")
        f.write(f"Ensemble RMSE: {performance_metrics['ensemble']['rmse']:.2f}\n")
        f.write(f"Ensemble MAE: {performance_metrics['ensemble']['mae']:.2f}\n\n")
        
        f.write(f"FEATURES ({len(feature_columns)}):\n")
        f.write(f"-" * 30 + "\n")
        for i, col in enumerate(feature_columns, 1):
            f.write(f"{i}. {col}\n")
        f.write(f"\n")
        
        f.write(f"USAGE:\n")
        f.write(f"-" * 30 + "\n")
        f.write(f"1. Load model: joblib.load('{model_path}')\n")
        f.write(f"2. Load features: open('{feature_path}', 'r').read().splitlines()\n")
        f.write(f"3. Predict adjustment ratio: adjustment_ratio = model.predict(features)\n")
        f.write(f"4. Final prediction = GRU_pred * (1 + adjustment_ratio)\n")
        f.write(f"5. VÃ­ dá»¥: adjustment_ratio = 0.05 â†’ tÄƒng 5%\n")
        f.write(f"6. VÃ­ dá»¥: adjustment_ratio = -0.03 â†’ giáº£m 3%\n")
    
    print(f"âœ… ÄÃ£ lÆ°u model info táº¡i: {info_path}")
    
    # 4. LÆ°u model parameters
    params_path = f'{output_dir}/model_parameters.txt'
    with open(params_path, 'w') as f:
        f.write(f"XGBOOST MODEL PARAMETERS\n")
        f.write(f"=" * 30 + "\n\n")
        if hasattr(model, 'get_params'):
            params = model.get_params()
            for key, value in params.items():
                f.write(f"{key}: {value}\n")
    
    print(f"âœ… ÄÃ£ lÆ°u model parameters táº¡i: {params_path}")
    
    print(f"\nğŸ‰ HOÃ€N THÃ€NH XUáº¤T MODEL!")
    print(f"ğŸ“ Model Ä‘Æ°á»£c lÆ°u trong thÆ° má»¥c: {output_dir}")
    print(f"ğŸ“‹ CÃ¡c file Ä‘Ã£ táº¡o:")
    print(f"   - xgb_model.pkl (Model Ä‘Ã£ train)")
    print(f"   - feature_columns.txt (Danh sÃ¡ch features)")
    print(f"   - model_info.txt (ThÃ´ng tin chi tiáº¿t)")
    print(f"   - model_parameters.txt (Hyperparameters)")
    print(f"\nğŸ’¡ LÆ¯U Ã QUAN TRá»ŒNG:")
    print(f"   - Model nÃ y dá»± Ä‘oÃ¡n ADJUSTMENT RATIO, khÃ´ng pháº£i giÃ¡ trá»‹ tuyá»‡t Ä‘á»‘i")
    print(f"   - Final prediction = GRU_pred * (1 + adjustment_ratio)")
    print(f"   - Adjustment ratio sáº½ scale theo magnitude cá»§a GRU prediction")
    
    return output_dir

def run_analysis(feature_type='absolute'):
    """
    Cháº¡y analysis cho XGBoost Adjustment model vá»›i má»™t loáº¡i feature cá»¥ thá»ƒ
    """
    print(f"ğŸ”„ Äang cháº¡y analysis cho XGBoost Adjustment model ({feature_type} features)...")
    
    try:
        # Load vÃ  prepare data
        df = load_and_prepare_data()
        if df is None:
            return None
            
        # Prepare features vÃ  target
        X, y, available_features = prepare_features_and_target(df, feature_type)
        if X is None:
            return None
            
        print(f"ğŸ“Š Dá»¯ liá»‡u: {len(X)} dÃ²ng, {len(X.columns)} features")
        print(f"ğŸ¯ Target: XGBoost há»c cÃ¡ch dá»± Ä‘oÃ¡n ADJUSTMENT (Weekly_Sales - GRU_pred)")
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, shuffle=False, random_state=42
        )
        
        print(f"ğŸ“š Train: {len(X_train)} dÃ²ng, Test: {len(X_test)} dÃ²ng")
        
        # Train XGBoost Adjustment model
        print("ğŸŒ³ Äang train XGBoost Adjustment model...")
        train_result = train_xgboost_model(X_train, y_train)
        if train_result is None:
            return None
            
        xgb_model, X_train, X_test, y_train, y_test = train_result
            
        # Evaluate model
        print("ğŸ“Š Äang Ä‘Ã¡nh giÃ¡ Adjustment model...")
        performance_metrics = evaluate_model(xgb_model, X_train, X_test, y_train, y_test, feature_type, df)
        
        # Calculate feature importance
        print("ğŸ” Äang tÃ­nh feature importance cho Adjustment model...")
        gain_df, perm_df, shap_df, shap_values, explainer = calculate_feature_importance(
            xgb_model, X_test, y_test, available_features, feature_type
        )
        
        # Create visualizations
        print("ğŸ“ˆ Äang táº¡o visualizations cho Adjustment model...")
        
        # Láº¥y GRU predictions tá»« df Ä‘á»ƒ truyá»n vÃ o create_visualizations
        gru_predictions = None
        if 'gru_pred' in df.columns:
            # Láº¥y GRU predictions tÆ°Æ¡ng á»©ng vá»›i test set
            test_indices = X_test.index
            gru_predictions = df.loc[test_indices, 'gru_pred']
            print(f"âœ… ÄÃ£ láº¥y GRU predictions cho visualization")
        
        create_visualizations(
            gain_df, perm_df, shap_df, shap_values, explainer,
            X_test, y_test, performance_metrics['ensemble_predictions'], 
            feature_type, available_features, gru_predictions
        )
        
        # Save results
        print("ğŸ’¾ Äang lÆ°u káº¿t quáº£ cho Adjustment model...")
        save_results(gain_df, perm_df, shap_df, y_test, 
                    performance_metrics['ensemble_predictions'], feature_type)
        
        # Print summary
        print_summary(gain_df, perm_df, shap_df, feature_type)
        
        # Export model
        export_model(xgb_model, available_features, feature_type, performance_metrics)
        
        return {
            'performance': performance_metrics,
            'gain_df': gain_df,
            'perm_df': perm_df,
            'shap_df': shap_df
        }
        
    except Exception as e:
        print(f"âŒ Lá»—i khi cháº¡y analysis cho Adjustment model: {e}")
        import traceback
        traceback.print_exc()
        return None

def compare_approaches():
    """
    So sÃ¡nh cáº£ hai approaches cho XGBoost Adjustment model
    """
    print("ğŸ”„ Äang cháº¡y cáº£ hai approaches Ä‘á»ƒ so sÃ¡nh XGBoost Adjustment model...")
    print("=" * 60)
    
    # Cháº¡y analysis cho absolute values
    print("\n1ï¸âƒ£ PHÆ¯Æ NG PHÃP 1: ABSOLUTE VALUES")
    print("-" * 40)
    results_absolute = run_analysis('absolute')
    
    if results_absolute is None:
        print("âŒ KhÃ´ng thá»ƒ cháº¡y absolute values approach")
        return
    
    # Cháº¡y analysis cho relative changes
    print("\n2ï¸âƒ£ PHÆ¯Æ NG PHÃP 2: RELATIVE CHANGES")
    print("-" * 40)
    results_relative = run_analysis('relative')
    
    if results_relative is None:
        print("âŒ KhÃ´ng thá»ƒ cháº¡y relative changes approach")
        return
    
    # So sÃ¡nh káº¿t quáº£
    print("\nğŸ† SO SÃNH Káº¾T QUáº¢ XGBOOST ADJUSTMENT MODEL:")
    print("=" * 60)
    
    print(f"\nğŸ¯ GRU Model (Baseline):")
    print(f"   Absolute RÂ²: {results_absolute['performance']['gru']['r2']:.4f}")
    print(f"   Relative RÂ²: {results_relative['performance']['gru']['r2']:.4f}")
    
    print(f"\nğŸŒ³ XGBoost Model (Adjustment):")
    print(f"   Absolute RÂ²: {results_absolute['performance']['xgb']['r2']:.4f}")
    print(f"   Relative RÂ²: {results_relative['performance']['xgb']['r2']:.4f}")
    
    print(f"\nğŸš€ Ensemble Model (GRU + XGBoost Adjustment):")
    print(f"   Absolute RÂ²: {results_absolute['performance']['ensemble']['r2']:.4f}")
    print(f"   Relative RÂ²: {results_relative['performance']['ensemble']['r2']:.4f}")
    
    print(f"\nğŸ“Š RMSE Comparison:")
    print(f"   Absolute: {results_absolute['performance']['ensemble']['rmse']:.2f}")
    print(f"   Relative: {results_relative['performance']['ensemble']['rmse']:.2f}")
    
    # XÃ¡c Ä‘á»‹nh approach tá»‘t hÆ¡n
    if results_absolute['performance']['ensemble']['r2'] > results_relative['performance']['ensemble']['r2']:
        print(f"\nâœ… ABSOLUTE VALUES hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n cho Adjustment model!")
        print(f"   RÂ² improvement: {results_absolute['performance']['ensemble']['r2'] - results_relative['performance']['ensemble']['r2']:.4f}")
    else:
        print(f"\nâœ… RELATIVE CHANGES hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n cho Adjustment model!")
        print(f"   RÂ² improvement: {results_relative['performance']['ensemble']['r2'] - results_absolute['performance']['ensemble']['r2']:.4f}")
    
    # So sÃ¡nh vá»›i GRU baseline
    print(f"\nğŸ“ˆ ENSEMBLE vs GRU BASELINE:")
    print(f"   Absolute improvement: {results_absolute['performance']['ensemble']['r2'] - results_absolute['performance']['gru']['r2']:.4f}")
    print(f"   Relative improvement: {results_relative['performance']['ensemble']['r2'] - results_relative['performance']['gru']['r2']:.4f}")
    
    print(f"\nğŸ‰ HOÃ€N THÃ€NH SO SÃNH XGBOOST ADJUSTMENT MODEL!")
    print(f"ğŸ“ Káº¿t quáº£ Ä‘Æ°á»£c lÆ°u trong:")
    print(f"   - output_absolute/")
    print(f"   - output_relative/")
    print(f"\nğŸ’¡ LÆ°u Ã½: Model nÃ y dá»± Ä‘oÃ¡n ADJUSTMENT, khÃ´ng pháº£i Weekly_Sales trá»±c tiáº¿p")

def main():
    """
    Main function vá»›i menu lá»±a chá»n
    """
    print("ğŸš€ WALMART SALES PREDICTION - XGBOOST ADJUSTMENT RATIO MODEL")
    print("=" * 60)
    print("ğŸ¯ MÃ´ hÃ¬nh: GRU + XGBoost Adjustment Ratio")
    print("ğŸ“Š GRU: Dá»± Ä‘oÃ¡n doanh thu dá»±a vÃ o lá»‹ch sá»­ 10 tuáº§n")
    print("ğŸŒ³ XGBoost: Äiá»u chá»‰nh GRU predictions theo tá»· lá»‡ dá»±a trÃªn external factors")
    print("ğŸ”„ Final Prediction = GRU_pred * (1 + adjustment_ratio)")
    print("ğŸ’¡ Adjustment ratio sáº½ scale theo magnitude cá»§a GRU prediction")
    print("=" * 60)
    
    while True:
        print("\nğŸ“‹ CHá»ŒN PHÆ¯Æ NG PHÃP:")
        print("1. Absolute Values (GiÃ¡ trá»‹ tuyá»‡t Ä‘á»‘i) - XGBoost Adjustment Ratio")
        print("2. Relative Changes (Thay Ä‘á»•i tÆ°Æ¡ng Ä‘á»‘i) - XGBoost Adjustment Ratio")
        print("3. So sÃ¡nh cáº£ hai approaches cho XGBoost Adjustment Ratio model")
        print("4. Xuáº¥t XGBoost Adjustment Ratio model (Relative Changes)")
        print("5. ThoÃ¡t")
        
        choice = input("\nğŸ‘‰ Nháº­p lá»±a chá»n (1-5) cho XGBoost Adjustment Ratio model: ").strip()
        
        if choice == '1':
            print("\nğŸ”„ Äang cháº¡y XGBoost Adjustment Ratio model vá»›i Absolute Values approach...")
            run_analysis('absolute')
            break
        elif choice == '2':
            print("\nğŸ”„ Äang cháº¡y XGBoost Adjustment Ratio model vá»›i Relative Changes approach...")
            run_analysis('relative')
            break
        elif choice == '3':
            print("\nğŸ”„ Äang cháº¡y cáº£ hai approaches Ä‘á»ƒ so sÃ¡nh XGBoost Adjustment Ratio model...")
            compare_approaches()
            break
        elif choice == '4':
            print("\nğŸ”„ Äang xuáº¥t XGBoost Adjustment Ratio model (Relative Changes)...")
            print("ğŸ’¡ Model nÃ y sáº½ dá»± Ä‘oÃ¡n ADJUSTMENT RATIO cho GRU predictions")
            print("ğŸ’¡ Final prediction = GRU_pred * (1 + adjustment_ratio)")
            # Cháº¡y analysis vÃ  xuáº¥t model
            results = run_analysis('relative')
            if results:
                print("âœ… ÄÃ£ xuáº¥t XGBoost Adjustment Ratio model thÃ nh cÃ´ng!")
                print("ğŸ“‹ CÃ¡ch sá»­ dá»¥ng:")
                print("   1. GRU dá»± Ä‘oÃ¡n doanh thu cÆ¡ báº£n")
                print("   2. XGBoost dá»± Ä‘oÃ¡n adjustment ratio dá»±a trÃªn external factors")
                print("   3. Final = GRU_pred * (1 + adjustment_ratio)")
                print("\nğŸ’¡ LÆ°u Ã½: Model nÃ y dá»± Ä‘oÃ¡n ADJUSTMENT RATIO, khÃ´ng pháº£i Weekly_Sales trá»±c tiáº¿p")
                print("ğŸ’¡ Adjustment ratio sáº½ scale theo magnitude cá»§a GRU prediction")
            break
        elif choice == '5':
            print("ğŸ‘‹ Táº¡m biá»‡t!")
            break
        else:
            print("âŒ Lá»±a chá»n khÃ´ng há»£p lá»‡. Vui lÃ²ng chá»n 1-5 cho XGBoost Adjustment Ratio model.")

if __name__ == "__main__":
    main()
